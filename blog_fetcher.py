#!/usr/bin/env python3
"""
ğŸ“° åšå®¢æ–‡ç« æŠ“å–å™¨
è·å– Anthropicã€DeepMindã€OpenAI ç­‰å®éªŒå®¤çš„æœ€æ–°åšå®¢
"""

import feedparser
import requests
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from bs4 import BeautifulSoup


class BlogFetcher:
    """åšå®¢æ–‡ç« æŠ“å–å™¨"""

    # RSS æºé…ç½®ï¼ˆåªä¿ç•™æœ‰æ´»è·ƒ RSS çš„æºï¼‰
    RSS_SOURCES = {
        # ============ Hacker Newsï¼ˆAI ç›¸å…³ï¼‰ ============
        'hn_ai': {
            'name': 'Hacker News (AI/ML)',
            'rss_url': 'https://hnrss.org/frontpage?q=AI+OR+LLM+OR+machine+learning+OR+deep+learning+OR+GPT+OR+transformer',
            'base_url': 'https://news.ycombinator.com'
        },
        'hn_best': {
            'name': 'Hacker News (Best)',
            'rss_url': 'https://hnrss.org/best',
            'base_url': 'https://news.ycombinator.com'
        },

        # ============ é¡¶çº§ä¸ªäººç ”ç©¶è€…åšå®¢ ============
        'karpathy': {
            'name': 'Andrej Karpathy',
            'rss_url': 'https://karpathy.github.io/feed.xml',
            'base_url': 'https://karpathy.github.io'
        },
        'simon_willison': {
            'name': 'Simon Willison (LLM å·¥å…·é“¾)',
            'rss_url': 'https://simonwillison.net/atom/everything/',
            'base_url': 'https://simonwillison.net'
        },
        'tim_dettmers': {
            'name': 'Tim Dettmers (é‡åŒ–/é«˜æ•ˆè®­ç»ƒ)',
            'rss_url': 'https://timdettmers.com/feed/',
            'base_url': 'https://timdettmers.com'
        },
        'chip_huyen': {
            'name': 'Chip Huyen (MLOps/æ•°æ®)',
            'rss_url': 'https://huyenchip.com/feed.xml',
            'base_url': 'https://huyenchip.com'
        },
        'jay_alammar': {
            'name': 'Jay Alammar (Transformer å¯è§†åŒ–)',
            'rss_url': 'https://jalammar.github.io/feed.xml',
            'base_url': 'https://jalammar.github.io'
        },
        'colah': {
            'name': 'Christopher Olah (Anthropic)',
            'rss_url': 'https://colah.github.io/rss.xml',
            'base_url': 'https://colah.github.io'
        },

        # ============ é«˜è´¨é‡ç¤¾åŒº/æœŸåˆŠ ============
        'lesswrong': {
            'name': 'LessWrong (AI Alignment)',
            'rss_url': 'https://www.lesswrong.com/feed.xml',
            'base_url': 'https://www.lesswrong.com'
        },
        'the_gradient': {
            'name': 'The Gradient (AI æ·±åº¦åˆ†æ)',
            'rss_url': 'https://thegradient.pub/rss/',
            'base_url': 'https://thegradient.pub'
        },
        'towards_data_science': {
            'name': 'Towards Data Science',
            'rss_url': 'https://towardsdatascience.com/feed',
            'base_url': 'https://towardsdatascience.com'
        },
        'ml_mastery': {
            'name': 'Machine Learning Mastery',
            'rss_url': 'https://machinelearningmastery.com/feed/',
            'base_url': 'https://machinelearningmastery.com'
        },
        'mit_tech_review': {
            'name': 'MIT Technology Review',
            'rss_url': 'https://www.technologyreview.com/feed/',
            'base_url': 'https://www.technologyreview.com'
        },

        # ============ é¡¶çº§å®éªŒå®¤/æœºæ„åšå®¢ ============
        'openai': {
            'name': 'OpenAI',
            'rss_url': 'https://openai.com/blog/rss.xml',
            'base_url': 'https://openai.com'
        },
        'anthropic': {
            'name': 'Anthropic',
            'rss_url': 'https://www.anthropic.com/rss',
            'base_url': 'https://www.anthropic.com'
        },
        'deepmind': {
            'name': 'DeepMind',
            'rss_url': 'https://deepmind.google/discover/blog/feed/',
            'base_url': 'https://deepmind.google'
        },
        'google_ai': {
            'name': 'Google AI',
            'rss_url': 'https://blog.google/technology/ai/rss/',
            'base_url': 'https://blog.google'
        },
        'meta_ai': {
            'name': 'Meta AI (FAIR)',
            'rss_url': 'https://ai.meta.com/blog/rss/',
            'base_url': 'https://ai.meta.com'
        },
        'microsoft_research': {
            'name': 'Microsoft Research',
            'rss_url': 'https://www.microsoft.com/en-us/research/blog/rss/',
            'base_url': 'https://www.microsoft.com/en-us/research/blog/'
        },
        'nvidia': {
            'name': 'NVIDIA AI Blog',
            'rss_url': 'https://blogs.nvidia.com/feed/',
            'base_url': 'https://blogs.nvidia.com'
        },
        'huggingface': {
            'name': 'Hugging Face Blog',
            'rss_url': 'https://huggingface.co/blog/feed.xml',
            'base_url': 'https://huggingface.co/blog'
        },
        'bair': {
            'name': 'BAIR (Berkeley AI Research)',
            'rss_url': 'https://bair.berkeley.edu/blog/feed.xml',
            'base_url': 'https://bair.berkeley.edu/blog'
        },
        'google_research': {
            'name': 'Google Research Blog',
            'rss_url': 'https://blog.research.google/feeds/posts/default',
            'base_url': 'https://blog.research.google'
        },
        'salesforce_ai': {
            'name': 'Salesforce AI Research',
            'rss_url': 'https://engineering.salesforce.com/rss/',
            'base_url': 'https://engineering.salesforce.com'
        },
    }

    def __init__(self, days_back: int = 7, max_articles: int = 5):
        """
        åˆå§‹åŒ–æŠ“å–å™¨

        Args:
            days_back: è·å–æœ€è¿‘å‡ å¤©çš„æ–‡ç« 
            max_articles: æ¯ä¸ªæºæœ€å¤šè·å–å¤šå°‘ç¯‡æ–‡ç« 
        """
        self.days_back = days_back
        self.max_articles = max_articles
        self.session = requests.Session()

    def fetch_blogs(self, sources: List[str] = None, fetch_full_content: bool = True) -> List[Dict]:
        """
        è·å–åšå®¢æ–‡ç« 

        Args:
            sources: è¦è·å–çš„æºåˆ—è¡¨ï¼Œå¦‚ ['anthropic', 'deepmind']ï¼ŒNone è¡¨ç¤ºè·å–æ‰€æœ‰
            fetch_full_content: æ˜¯å¦è·å–å…¨æ–‡å†…å®¹

        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        if sources is None:
            sources = list(self.RSS_SOURCES.keys())

        all_articles = []
        cutoff_date = datetime.now() - timedelta(days=self.days_back)

        for source_key in sources:
            if source_key not in self.RSS_SOURCES:
                print(f"âš ï¸  æœªçŸ¥çš„æº: {source_key}")
                continue

            source_config = self.RSS_SOURCES[source_key]
            print(f"ğŸ“° è·å– {source_config['name']} åšå®¢...")

            articles = self._fetch_from_rss(source_key, source_config, cutoff_date)

            # è·å–å…¨æ–‡å†…å®¹
            if fetch_full_content:
                for article in articles:
                    full_content = self._fetch_full_content(article)
                    if full_content:
                        article['full_content'] = full_content

            all_articles.extend(articles)

        # å»é‡ï¼šæŒ‰é“¾æ¥å»é‡ï¼Œé¿å…åŒä¸€ç¯‡æ–‡ç« é‡å¤å‡ºç°
        seen_links = set()
        unique_articles = []
        for article in all_articles:
            link = article.get('link', '')
            if link and link not in seen_links:
                seen_links.add(link)
                unique_articles.append(article)

        # æŒ‰æ—¶é—´æ’åºï¼ˆç”¨ feedparser è§£ææ—¥æœŸï¼Œè€Œéå­—ç¬¦ä¸²æ¯”è¾ƒï¼‰
        def sort_key(x):
            pub = x.get('published', '')
            for fmt in ['%a, %d %b %Y %H:%M:%S %z', '%Y-%m-%dT%H:%M:%S%z',
                         '%Y-%m-%dT%H:%M:%SZ', '%Y-%m-%dT%H:%M:%S', '%Y-%m-%d']:
                try:
                    return datetime.strptime(pub.strip(), fmt).replace(tzinfo=None)
                except (ValueError, TypeError):
                    continue
            return datetime.min

        unique_articles.sort(key=sort_key, reverse=True)

        print(f"âœ… è·å–åˆ° {len(unique_articles)} ç¯‡åšå®¢æ–‡ç« ï¼ˆå»é‡å‰ {len(all_articles)} ç¯‡ï¼‰")
        return unique_articles

    def _fetch_full_content(self, article: Dict) -> Optional[str]:
        """è·å–åšå®¢æ–‡ç« çš„å…¨æ–‡å†…å®¹"""
        try:
            url = article['link']
            response = self.session.get(url, timeout=30)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # ç§»é™¤ä¸éœ€è¦çš„æ ‡ç­¾
            for script in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                script.decompose()

            # æå–ä¸»è¦å†…å®¹
            content = soup.get_text(separator='\n', strip=True)

            # æ¸…ç†ç©ºç™½è¡Œ
            lines = [line.strip() for line in content.split('\n')]
            lines = [line for line in lines if line and len(line) > 20]

            full_text = '\n'.join(lines[:500])  # å–å‰ 500 è¡Œ

            return full_text if len(full_text) > 200 else None

        except Exception as e:
            print(f"  âš ï¸  è·å–å…¨æ–‡å¤±è´¥: {e}")
            return None

    def _parse_entry_date(self, entry) -> Optional[datetime]:
        """ä» RSS entry è§£æå‘å¸ƒæ—¶é—´ï¼Œä¼˜å…ˆä½¿ç”¨ feedparser å·²è§£æçš„ struct_time"""
        # feedparser ä¼šè‡ªåŠ¨è§£ææ—¥æœŸåˆ° published_parsed / updated_parsed
        parsed = entry.get('published_parsed') or entry.get('updated_parsed')
        if parsed:
            try:
                return datetime(*parsed[:6])
            except Exception:
                pass

        # å›é€€ï¼šæ‰‹åŠ¨è§£æåŸå§‹æ—¥æœŸå­—ç¬¦ä¸²
        published = entry.get('published', entry.get('updated', ''))
        if not published:
            return None

        date_formats = [
            '%a, %d %b %Y %H:%M:%S %z',
            '%a, %d %b %Y %H:%M:%S %Z',
            '%Y-%m-%dT%H:%M:%S%z',
            '%Y-%m-%dT%H:%M:%S.%f%z',
            '%Y-%m-%dT%H:%M:%SZ',
            '%Y-%m-%dT%H:%M:%S',
            '%Y-%m-%d %H:%M:%S',
            '%Y-%m-%d'
        ]
        for fmt in date_formats:
            try:
                return datetime.strptime(published.strip(), fmt)
            except (ValueError, TypeError):
                continue
        return None

    def _fetch_from_rss(self, source_key: str, source_config: Dict, cutoff_date: datetime) -> List[Dict]:
        """ä» RSS æºè·å–æ–‡ç« """
        try:
            rss_url = source_config.get('rss_url', '')
            if not rss_url:
                print(f"  â””â”€ âš ï¸  è¯¥æºæ²¡æœ‰é…ç½® RSS URL")
                return []

            feed = feedparser.parse(rss_url)

            if feed.bozo and not feed.entries:
                print(f"  â””â”€ âŒ RSS è§£æå¤±è´¥: {feed.bozo_exception}")
                return []

            if not feed.entries:
                print(f"  â””â”€ âš ï¸  RSS è¿”å› 0 æ¡ç›®")
                return []

            # å…ˆéå†æ‰€æœ‰æ¡ç›®ï¼Œè§£ææ—¥æœŸå¹¶è¿‡æ»¤ï¼Œå†æŒ‰æ—¶é—´æ’åºå– top N
            candidates = []
            for entry in feed.entries:
                pub_date = self._parse_entry_date(entry)
                published = entry.get('published', entry.get('updated', ''))

                # æ—¶é—´è¿‡æ»¤ï¼šæœ‰æ—¥æœŸçš„æŒ‰æ—¥æœŸè¿‡æ»¤ï¼Œæ— æ—¥æœŸçš„è·³è¿‡ï¼ˆé¿å…æ··å…¥æ—§æ–‡ç« ï¼‰
                if pub_date:
                    pub_date_naive = pub_date.replace(tzinfo=None) if pub_date.tzinfo else pub_date
                    if pub_date_naive < cutoff_date:
                        continue
                else:
                    # æ— æ³•è§£ææ—¥æœŸï¼Œè·³è¿‡è¿™æ¡ï¼Œé¿å…æ”¶å…¥ä¸ç¡®å®šæ—¶é—´çš„æ—§æ–‡ç« 
                    continue

                # æå–æ‘˜è¦
                summary = entry.get('summary', entry.get('description', ''))
                if summary:
                    soup = BeautifulSoup(summary, 'html.parser')
                    summary = soup.get_text()[:500]

                candidates.append({
                    'title': entry.get('title', ''),
                    'link': entry.get('link', ''),
                    'summary': summary,
                    'published': published,
                    'pub_date': pub_date_naive,
                    'source': source_config['name'],
                    'source_key': source_key
                })

            # æŒ‰æ—¥æœŸå€’åºæ’åºï¼Œå–æœ€æ–°çš„ max_articles ç¯‡
            candidates.sort(key=lambda x: x['pub_date'], reverse=True)
            articles = candidates[:self.max_articles]

            # ç§»é™¤å†…éƒ¨æ’åºå­—æ®µ
            for a in articles:
                del a['pub_date']

            print(f"  â””â”€ {len(articles)} ç¯‡ï¼ˆå…± {len(candidates)} ç¯‡åœ¨æ—¶é—´èŒƒå›´å†…ï¼‰")
            return articles

        except Exception as e:
            print(f"  â””â”€ âŒ è·å–å¤±è´¥: {e}")
            return []


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    fetcher = BlogFetcher(days_back=7, max_articles=3)
    blogs = fetcher.fetch_blogs(['anthropic', 'deepmind'])

    print("\n" + "=" * 60)
    print("ğŸ“° æœ€æ–°åšå®¢æ–‡ç« ")
    print("=" * 60)

    for i, blog in enumerate(blogs, 1):
        print(f"\n{i}. {blog['title']}")
        print(f"   æ¥æº: {blog['source']}")
        print(f"   å‘å¸ƒ: {blog['published']}")
        print(f"   é“¾æ¥: {blog['link']}")
        print(f"   æ‘˜è¦: {blog['summary'][:100]}...")
