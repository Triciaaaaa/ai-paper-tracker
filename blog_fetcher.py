#!/usr/bin/env python3
"""
ğŸ“° åšå®¢æ–‡ç« æŠ“å–å™¨
è·å– Anthropicã€DeepMindã€OpenAI ç­‰å®éªŒå®¤çš„æœ€æ–°åšå®¢
"""

import feedparser
import requests
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from bs4 import BeautifulSoup


class BlogFetcher:
    """åšå®¢æ–‡ç« æŠ“å–å™¨"""

    # RSS æºé…ç½®
    RSS_SOURCES = {
        # ============ é¡¶çº§ä¸ªäººåšå®¢ ============
        'ilya': {
            'name': 'Ilya Sutskever',
            'rss_url': 'https://www.ilyasuresh.com/rss',
            'base_url': 'https://www.ilyasuresh.com'
        },
        'sutton': {
            'name': 'Richard Sutton',
            'rss_url': 'https://gradientflow.org/rss/',
            'base_url': 'https://www.cs.ualberta.ca/~sutton/'
        },
        'karpathy': {
            'name': 'Andrej Karpathy',
            'rss_url': 'https://karpathy.github.io/feed.xml',
            'base_url': 'https://karpathy.github.io'
        },
        'le_cun': {
            'name': 'Yann LeCun',
            'rss_url': 'https://www.facebook.com/feeds/page.php?id=35561552908&format=rss20',
            'base_url': 'https://www.facebook.com/ylecun1960'
        },
        'hinton': {
            'name': 'Geoffrey Hinton',
            'rss_url': '',
            'base_url': 'https://www.cs.toronto.edu/~hinton/'
        },
        'bengio': {
            'name': 'Yoshua Bengio',
            'rss_url': '',
            'base_url': 'https://yoshuabengio.org/'
        },
        'schramowski': {
            'name': 'Simon Schmickler (Vijay P. ç­‰äºº)',
            'rss_url': 'https://www.alignmentforum.org/feed',
            'base_url': 'https://www.alignmentforum.org'
        },

        # ============ ä¸ªäººç ”ç©¶åšå®¢ï¼ˆé«˜è´¨é‡ï¼‰ ============
        'lesswrong': {
            'name': 'LessWrong (AI Alignment ç¤¾åŒº)',
            'rss_url': 'https://www.lesswrong.com/feed.xml',
            'base_url': 'https://www.lesswrong.com'
        },
        'distill': {
            'name': 'Distill.pub (äº¤äº’å¼ç§‘å­¦å‡ºç‰ˆç‰©)',
            'rss_url': 'https://distill.pub/rss.xml',
            'base_url': 'https://distill.pub'
        },
        'jeremykun': {
            'name': 'Jeremy Kun (Math âˆ© Programming)',
            'rss_url': 'https://jeremykun.com/feed/',
            'base_url': 'https://jeremykun.com'
        },
        'colah': {
            'name': 'Christopher Olah (Anthropic, ç¥ç»ç½‘ç»œå¯è§†åŒ–)',
            'rss_url': 'https://colah.github.io/rss.xml',
            'base_url': 'https://colah.github.io'
        },
        'weng': {
            'name': 'Lilian Weng (OpenAI å®‰å…¨ç ”ç©¶)',
            'rss_url': 'https://lilianweng.github.io/feed.xml',
            'base_url': 'https://lilianweng.github.io'
        },

        # ============ AI åª’ä½“å’ŒæœŸåˆŠ ============
        'mit_tech_review': {
            'name': 'MIT Technology Review',
            'rss_url': 'https://www.technologyreview.com/feed/',
            'base_url': 'https://www.technologyreview.com'
        },

        # ============ é¡¶çº§ç ”ç©¶æœºæ„ ============
        'ssi': {
            'name': 'Schmidt Futures (SSI)',
            'rss_url': 'https://www.schmidtfutures.org/news/feed/',
            'base_url': 'https://www.schmidtfutures.org'
        },
        'thinking_machines': {
            'name': 'Thinking Machines',
            'rss_url': '',
            'base_url': 'https://www.thinkingmachines.com'
        },
        'openai': {
            'name': 'OpenAI',
            'rss_url': 'https://openai.com/blog/rss.xml',
            'base_url': 'https://openai.com'
        },
        'anthropic': {
            'name': 'Anthropic',
            'rss_url': 'https://www.anthropic.com/rss',
            'base_url': 'https://www.anthropic.com'
        },
        'deepmind': {
            'name': 'DeepMind',
            'rss_url': 'https://deepmind.google/discover/blog/feed/',
            'base_url': 'https://deepmind.google'
        },
        'google_ai': {
            'name': 'Google AI',
            'rss_url': 'https://blog.google/technology/ai/rss/',
            'base_url': 'https://blog.google'
        },
        'meta_ai': {
            'name': 'Meta AI (FAIR)',
            'rss_url': 'https://ai.meta.com/blog/rss/',
            'base_url': 'https://ai.meta.com'
        },
        'microsoft_research': {
            'name': 'Microsoft Research',
            'rss_url': 'https://www.microsoft.com/en-us/research/blog/rss/',
            'base_url': 'https://www.microsoft.com/en-us/research/blog/'
        },
        'salesforce_ai': {
            'name': 'Salesforce AI Research',
            'rss_url': 'https://engineering.salesforce.com/rss/',
            'base_url': 'https://engineering.salesforce.com'
        },
        'openresearch': {
            'name': 'OpenResearch',
            'rss_url': 'https://www.openresearch.org/blog/feed/',
            'base_url': 'https://www.openresearch.org'
        },
        'mira': {
            'name': 'Mira Research (Yann LeCun çš„æ–°å…¬å¸)',
            'rss_url': '',
            'base_url': 'https://www.mira-research.org'
        }
    }

    def __init__(self, days_back: int = 7, max_articles: int = 5):
        """
        åˆå§‹åŒ–æŠ“å–å™¨

        Args:
            days_back: è·å–æœ€è¿‘å‡ å¤©çš„æ–‡ç« 
            max_articles: æ¯ä¸ªæºæœ€å¤šè·å–å¤šå°‘ç¯‡æ–‡ç« 
        """
        self.days_back = days_back
        self.max_articles = max_articles
        self.session = requests.Session()

    def fetch_blogs(self, sources: List[str] = None, fetch_full_content: bool = True) -> List[Dict]:
        """
        è·å–åšå®¢æ–‡ç« 

        Args:
            sources: è¦è·å–çš„æºåˆ—è¡¨ï¼Œå¦‚ ['anthropic', 'deepmind']ï¼ŒNone è¡¨ç¤ºè·å–æ‰€æœ‰
            fetch_full_content: æ˜¯å¦è·å–å…¨æ–‡å†…å®¹

        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        if sources is None:
            sources = list(self.RSS_SOURCES.keys())

        all_articles = []
        cutoff_date = datetime.now() - timedelta(days=self.days_back)

        for source_key in sources:
            if source_key not in self.RSS_SOURCES:
                print(f"âš ï¸  æœªçŸ¥çš„æº: {source_key}")
                continue

            source_config = self.RSS_SOURCES[source_key]
            print(f"ğŸ“° è·å– {source_config['name']} åšå®¢...")

            articles = self._fetch_from_rss(source_key, source_config, cutoff_date)

            # è·å–å…¨æ–‡å†…å®¹
            if fetch_full_content:
                for article in articles:
                    full_content = self._fetch_full_content(article)
                    if full_content:
                        article['full_content'] = full_content

            all_articles.extend(articles)

        # æŒ‰æ—¶é—´æ’åº
        all_articles.sort(key=lambda x: x.get('published', ''), reverse=True)

        print(f"âœ… è·å–åˆ° {len(all_articles)} ç¯‡åšå®¢æ–‡ç« ")
        return all_articles

    def _fetch_full_content(self, article: Dict) -> Optional[str]:
        """è·å–åšå®¢æ–‡ç« çš„å…¨æ–‡å†…å®¹"""
        try:
            url = article['link']
            response = self.session.get(url, timeout=30)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # ç§»é™¤ä¸éœ€è¦çš„æ ‡ç­¾
            for script in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                script.decompose()

            # æå–ä¸»è¦å†…å®¹
            content = soup.get_text(separator='\n', strip=True)

            # æ¸…ç†ç©ºç™½è¡Œ
            lines = [line.strip() for line in content.split('\n')]
            lines = [line for line in lines if line and len(line) > 20]

            full_text = '\n'.join(lines[:500])  # å–å‰ 500 è¡Œ

            return full_text if len(full_text) > 200 else None

        except Exception as e:
            print(f"  âš ï¸  è·å–å…¨æ–‡å¤±è´¥: {e}")
            return None

    def _fetch_from_rss(self, source_key: str, source_config: Dict, cutoff_date: datetime) -> List[Dict]:
        """ä» RSS æºè·å–æ–‡ç« """
        try:
            rss_url = source_config.get('rss_url', '')
            if not rss_url:
                print(f"  â””â”€ âš ï¸  è¯¥æºæ²¡æœ‰é…ç½® RSS URL")
                return []

            feed = feedparser.parse(rss_url)
            articles = []

            for entry in feed.entries[:self.max_articles]:
                # è§£æå‘å¸ƒæ—¶é—´
                published = entry.get('published', entry.get('updated', ''))
                pub_date = None

                if published:
                    # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
                    date_formats = [
                        '%a, %d %b %Y %H:%M:%S %z',
                        '%a, %d %b %Y %H:%M:%S %Z',
                        '%Y-%m-%dT%H:%M:%S%z',
                        '%Y-%m-%dT%H:%M:%SZ',
                        '%Y-%m-%dT%H:%M:%S',
                        '%Y-%m-%d'
                    ]

                    for fmt in date_formats:
                        try:
                            pub_date = datetime.strptime(published.strip(), fmt)
                            break
                        except:
                            continue

                # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´èŒƒå›´å†…
                if pub_date:
                    # ç§»é™¤æ—¶åŒºä¿¡æ¯ä»¥ä¾¿æ¯”è¾ƒ
                    if pub_date.tzinfo:
                        cutoff_date_with_tz = cutoff_date.replace(tzinfo=pub_date.tzinfo)
                    else:
                        cutoff_date_with_tz = cutoff_date

                    if pub_date < cutoff_date_with_tz:
                        continue

                # æå–æ‘˜è¦
                summary = entry.get('summary', entry.get('description', ''))
                # æ¸…ç† HTML æ ‡ç­¾
                if summary:
                    soup = BeautifulSoup(summary, 'html.parser')
                    summary = soup.get_text()[:500]

                article = {
                    'title': entry.get('title', ''),
                    'link': entry.get('link', ''),
                    'summary': summary,
                    'published': published,
                    'source': source_config['name'],
                    'source_key': source_key
                }

                articles.append(article)

            print(f"  â””â”€ {len(articles)} ç¯‡")
            return articles

        except Exception as e:
            print(f"  â””â”€ âŒ è·å–å¤±è´¥: {e}")
            return []


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    fetcher = BlogFetcher(days_back=7, max_articles=3)
    blogs = fetcher.fetch_blogs(['anthropic', 'deepmind'])

    print("\n" + "=" * 60)
    print("ğŸ“° æœ€æ–°åšå®¢æ–‡ç« ")
    print("=" * 60)

    for i, blog in enumerate(blogs, 1):
        print(f"\n{i}. {blog['title']}")
        print(f"   æ¥æº: {blog['source']}")
        print(f"   å‘å¸ƒ: {blog['published']}")
        print(f"   é“¾æ¥: {blog['link']}")
        print(f"   æ‘˜è¦: {blog['summary'][:100]}...")
