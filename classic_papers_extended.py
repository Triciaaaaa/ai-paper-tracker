#!/usr/bin/env python3
"""
ğŸ“š ç»å…¸è®ºæ–‡æ¨¡å—ï¼ˆæ‰©å±•ç‰ˆï¼‰
ç²¾é€‰ AI é¢†åŸŸçš„ç»å…¸è®ºæ–‡ï¼ŒåŒ…å«é¢†åŸŸå…³é”®è¯è§£æ
"""

from typing import List, Dict, Optional


# ç»å…¸è®ºæ–‡åˆ—è¡¨ï¼ˆå¤§å¹…æ‰©å±•ï¼‰
CLASSIC_PAPERS = {
    'reinforcement_learning': [
        {
            'title': 'Reinforcement Learning: An Introduction (2nd Edition)',
            'authors': 'Richard S. Sutton, Andrew G. Barto',
            'year': '2018',
            'url': 'https://mitpress.mit.edu/books/reinforcement-learning-second-edition',
            'description': 'å¼ºåŒ–å­¦ä¹ é¢†åŸŸçš„åœ£ç»ï¼Œç³»ç»Ÿä»‹ç»äº† RL çš„ç†è®ºåŸºç¡€ï¼ŒåŒ…æ‹¬ TD å­¦ä¹ ã€ç­–ç•¥æ¢¯åº¦ç­‰æ ¸å¿ƒæ¦‚å¿µã€‚',
            'keywords': ['reinforcement learning', 'temporal difference', 'Q-learning', 'policy gradient', 'value function', 'exploration']
        },
        {
            'title': 'Human-level control through deep reinforcement learning',
            'authors': 'Mnih et al. (DeepMind)',
            'year': '2015',
            'url': 'https://www.nature.com/articles/nature14236',
            'description': 'DQN è®ºæ–‡ï¼Œæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„é‡Œç¨‹ç¢‘å·¥ä½œï¼Œå±•ç¤ºäº† AI å¯ä»¥é€šè¿‡ç«¯åˆ°ç«¯å­¦ä¹ è¾¾åˆ°äººç±»æ°´å¹³çš„æ§åˆ¶èƒ½åŠ›ã€‚',
            'keywords': ['DQN', 'deep RL', 'Q-learning', 'atari games', 'convolutional neural network']
        },
        {
            'title': 'Policy Gradient Methods for Reinforcement Learning with Function Approximation',
            'authors': 'Sutton, McAllester, Singh, Mansour',
            'year': '2000',
            'url': 'https://proceedings.neurips.cc/paper/2000/file/461271028c68e8f25be1b2a2fb309df9-Paper.pdf',
            'description': 'ç­–ç•¥æ¢¯åº¦æ–¹æ³•çš„ç†è®ºåŸºç¡€ï¼Œè¯æ˜äº†ç­–ç•¥æ¢¯åº¦æ–¹æ³•çš„æ”¶æ•›æ€§ã€‚',
            'keywords': ['policy gradient', 'function approximation', 'actor-critic', 'convergence']
        },
        {
            'title': 'Asynchronous Methods for Deep Reinforcement Learning',
            'authors': 'Mnih et al. (DeepMind)',
            'year': '2016',
            'url': 'https://arxiv.org/abs/1602.01783',
            'description': 'A3C ç®—æ³•ï¼Œå¼‚æ­¥Actor-Criticï¼Œè§£å†³äº†å¤§è§„æ¨¡åˆ†å¸ƒå¼ RL çš„è®­ç»ƒé—®é¢˜ã€‚',
            'keywords': ['A3C', 'asynchronous', 'actor-critic', 'distributed RL', 'parallel']
        },
        {
            'title': 'Proximal Policy Optimization Algorithms (PPO)',
            'authors': 'Schulman et al. (OpenAI)',
            'year': '2017',
            'url': 'https://arxiv.org/abs/1707.06347',
            'description': 'PPO ç®—æ³•ï¼Œå¹³è¡¡äº† sample å¤æ‚åº¦å’Œå®ç°å¤æ‚æ€§ï¼Œæˆä¸ºæœ€æµè¡Œçš„ RL ç®—æ³•ä¹‹ä¸€ã€‚',
            'keywords': ['PPO', 'trust region', 'policy optimization', 'clipped surrogate', 'sample efficiency']
        },
        {
            'title': 'Reward Shaping',
            'authors': 'Ng, Harada, Russell (UC Berkeley)',
            'year': '1999',
            'url': 'https://people.eecs.berkeley.edu/~pabbeel/cs287/npapers/99-shaping.pdf',
            'description': 'å¥–åŠ±å¡‘é€ ç†è®ºï¼Œè¯æ˜äº†å¦‚ä½•åœ¨ä¸æ”¹å˜æœ€ä¼˜ç­–ç•¥çš„å‰æä¸‹è®¾è®¡å¥–åŠ±å‡½æ•°ã€‚',
            'keywords': ['reward shaping', 'potential-based reward', 'reward hypothesis', 'optimal policy']
        }
    ],
    'alignment': [
        {
            'title': 'Concrete Problems in AI Safety',
            'authors': 'Amodei et al. (OpenAI)',
            'year': '2016',
            'url': 'https://arxiv.org/abs/1606.06565',
            'description': 'AI å®‰å…¨é¢†åŸŸçš„ç»å…¸è®ºæ–‡ï¼Œæå‡ºäº†å…·ä½“çš„ safety é—®é¢˜ï¼šé¿å…è´Ÿé¢å½±å“ã€å¥–åŠ±å¹²æ‰°ã€å¯æ‰©å±•çš„ç›‘ç£ã€å®‰å…¨æ¢ç´¢ç­‰ã€‚',
            'keywords': ['AI safety', 'reward hacking', 'side effects', 'scalable oversight', 'safe exploration']
        },
        {
            'title': 'Scalable Agent Alignment via Reward Modeling',
            'authors': 'Leike et al. (DeepMind)',
            'year': '2018',
            'url': 'https://arxiv.org/abs/1811.07871',
            'description': 'RLHF çš„åŸºç¡€è®ºæ–‡ä¹‹ä¸€ï¼Œæå‡ºäº†é€šè¿‡å¥–åŠ±å»ºæ¨¡æ¥å¯¹é½ Agent è¡Œä¸ºçš„æ–¹æ³•ã€‚',
            'keywords': ['RLHF', 'reward modeling', 'human feedback', 'agent alignment', 'preference learning']
        },
        {
            'title': 'Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback',
            'authors': 'Bai et al. (Anthropic)',
            'year': '2022',
            'url': 'https://arxiv.org/abs/2204.05862',
            'description': 'Constitutional AI çš„åŸºç¡€ï¼ŒAnthropic çš„æ ¸å¿ƒå·¥ä½œï¼Œé€šè¿‡ RLHF è®­ç»ƒå‡ºæœ‰ç”¨ä¸”æ— å®³çš„ AIã€‚',
            'keywords': ['RLHF', 'constitutional AI', 'harmlessness', 'helpfulness', 'HHH']
        },
        {
            'title': 'Language Models are Few-Shot Learners',
            'authors': 'Brown et al. (OpenAI)',
            'year': '2020',
            'url': 'https://arxiv.org/abs/2005.14165',
            'description': 'GPT-3 è®ºæ–‡ï¼Œå±•ç¤ºäº†å¤§è¯­è¨€æ¨¡å‹çš„å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œä¸º AI å¯¹é½ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚',
            'keywords': ['GPT-3', 'few-shot learning', 'in-context learning', 'language models', 'scaling laws']
        },
        {
            'title': 'Prima Facie Approximations of Value Learning',
            'authors': 'Uehara et al.',
            'year': '2020',
            'url': 'https://arxiv.org/abs/2010.08519',
            'description': 'ä»·å€¼å­¦ä¹ çš„åŸºç¡€ç†è®ºï¼Œåˆ†æäº† reward hacking é—®é¢˜ã€‚',
            'keywords': ['value learning', 'reward hacking', 'preference learning', 'approximation']
        }
    ],
    'ai4math': [
        {
            'title': 'Solving Olympiad Geometry without Human Demonstrations',
            'authors': 'Tao et al. (Google)',
            'year': '2024',
            'url': 'https://nature.com/articles/s41586-024-08067-6',
            'description': 'AlphaGeometryï¼ŒAI è§£æ•°å­¦é¢˜çš„é‡Œç¨‹ç¢‘ï¼Œè¾¾åˆ°äº†å›½é™…å¥¥æ•°å‡ ä½•é‡‘ç‰Œæ°´å¹³ã€‚',
            'keywords': ['AlphaGeometry', 'theorem proving', 'geometry', 'mathematical reasoning', 'synthetic data']
        },
        {
            'title': 'Advancing Mathematics by Guiding Large Language Models',
            'authors': 'Tao et al.',
            'year': '2024',
            'url': 'https://arxiv.org/abs/2312.06761',
            'description': 'é™¶å“²è½©å›¢é˜Ÿç”¨ LLM è¾…åŠ©æ•°å­¦ç ”ç©¶ï¼Œå±•ç¤ºäº† AI åœ¨æ•°å­¦å‘ç°ä¸­çš„æ½œåŠ›ã€‚',
            'keywords': ['LLM for math', 'mathematical discovery', 'formal proof', 'computer algebra']
        },
        {
            'title': 'Neural Theorem Provers: An Update',
            'authors': 'Polu et al.',
            'year': '2022',
            'url': 'https://arxiv.org/abs/2209.05777',
            'description': 'ç¥ç»å®šç†è¯æ˜å™¨çš„æœ€æ–°è¿›å±•ï¼ŒåŒ…æ‹¬åœ¨ Lean 4 ä¸­è¯æ˜æ•°å­¦å®šç†ã€‚',
            'keywords': ['theorem proving', 'formal verification', 'Lean', 'tactics', 'mathlib']
        },
        {
            'title': 'Mathematical Reasoning with Lean 4',
            'authors': 'Ullrich et al.',
            'year': '2024',
            'url': 'https://arxiv.org/abs/2312.06483',
            'description': 'Lean 4 æ•°å­¦æ¨ç†ç³»ç»Ÿçš„æœ€æ–°è¿›å±•ã€‚',
            'keywords': ['Lean 4', 'mathematical reasoning', 'formal proof assistant', 'proof automation']
        }
    ],
    'formal_verification': [
        {
            'title': 'Communicating Sequential Processes',
            'authors': 'Tony Hoare',
            'year': '1978',
            'url': 'https://www.cs.ox.ac.uk/files/3328/CSP.pdf',
            'description': 'CSP ç†è®ºï¼Œç”¨äºæè¿°å¹¶å‘ç³»ç»Ÿçš„é€šä¿¡è¡Œä¸ºï¼Œæ˜¯å½¢å¼åŒ–éªŒè¯çš„åŸºç¡€ç†è®ºã€‚',
            'keywords': ['CSP', 'concurrency', 'process algebra', 'formal methods', 'channels']
        },
        {
            'title': 'Computation Tree Logic (CTL)',
            'authors': 'Clarke, Emerson',
            'year': '1981',
            'url': 'https://doi.org/10.1145/322186.322201',
            'description': 'CTL æ¨¡å‹éªŒè¯çš„åŸºç¡€ï¼Œç”¨äºéªŒè¯æœ‰é™çŠ¶æ€ç³»ç»Ÿçš„æ€§è´¨ã€‚',
            'keywords': ['CTL', 'model checking', 'temporal logic', 'verification', 'state space']
        },
        {
            'title': 'Model Checking',
            'authors': 'Clarke, Grumberg, Peled',
            'year': '1999',
            'url': 'https://mitpress.mit.edu/books/model-checking/',
            'description': 'æ¨¡å‹éªŒè¯çš„ç»å…¸æ•™æï¼Œç³»ç»Ÿä»‹ç»äº†æ¨¡å‹éªŒè¯çš„ç†è®ºå’Œå®è·µã€‚',
            'keywords': ['model checking', 'temporal logic', 'model checking', 'verification', 'SPIN model checker']
        },
        {
            'title': 'The Temporal Logic of Reactive and Concurrent Systems: Specification and Verification',
            'authors': 'Manna, Pnueli',
            'year': '1992',
            'url': 'https://mitpress.mit.edu/books/temporal-logic/',
            'description': 'æ—¶åºé€»è¾‘çš„ç»å…¸è‘—ä½œï¼Œç”¨äºéªŒè¯ååº”å¼å’Œå¹¶å‘ç³»ç»Ÿçš„æ€§è´¨ã€‚',
            'keywords': ['temporal logic', 'reactive systems', 'concurrency', 'specification', 'verification']
        }
    ],
    'llm': [
        {
            'title': 'Attention Is All You Need',
            'authors': 'Vaswani et al.',
            'year': '2017',
            'url': 'https://arxiv.org/abs/1706.03762',
            'description': 'Transformer æ¶æ„çš„å¥ åŸºä¹‹ä½œï¼Œself-attention æœºåˆ¶æ”¹å˜äº† NLP å’Œ RL é¢†åŸŸã€‚',
            'keywords': ['Transformer', 'self-attention', 'attention mechanism', 'encoder-decoder', 'multi-head attention']
        },
        {
            'title': 'Language Models are Few-Shot Learners',
            'authors': 'Brown et al. (OpenAI)',
            'year': '2020',
            'url': 'https://arxiv.org/abs/2005.14165',
            'description': 'GPT-3 è®ºæ–‡ï¼Œå±•ç¤ºäº†å¤§è¯­è¨€æ¨¡å‹çš„ emergent èƒ½åŠ›ï¼ŒåŒ…æ‹¬ few-shot learningã€‚',
            'keywords': ['GPT-3', 'few-shot learning', 'in-context learning', 'emergent abilities', 'scaling']
        },
        {
            'title': 'Constitutional AI: Harmlessness from AI Feedback',
            'authors': 'Bai et al. (Anthropic)',
            'year': '2022',
            'url': 'https://arxiv.org/abs/2212.08073',
            'description': 'Constitutional AI çš„å®Œæ•´è®ºæ–‡ï¼Œæå‡ºé€šè¿‡ AI åé¦ˆæ¥è®­ç»ƒæ— å®³çš„ AIã€‚',
            'keywords': ['constitutional AI', 'AI feedback', 'harmlessness', 'RLAIF', 'critic']
        },
        {
            'title': 'Training Language Models to Follow Instructions with Human Feedback',
            'authors': 'Ouyang et al. (OpenAI)',
            'year': '2022',
            'url': 'https://arxiv.org/abs/2203.02155',
            'description': 'InstructGPT çš„è®ºæ–‡ï¼Œå±•ç¤ºäº†é€šè¿‡äººç±»åé¦ˆè®­ç»ƒè®©æ¨¡å‹éµå¾ªæŒ‡ä»¤ã€‚',
            'keywords': ['InstructGPT', 'instruction following', 'RLHF', 'fine-tuning', 'human feedback']
        }
    ],
    'information_theory': [
        {
            'title': 'A Mathematical Theory of Communication',
            'authors': 'Claude E. Shannon',
            'year': '1948',
            'url': 'https://people.math.harvard.edu/~ctm/home/text/others/shannon1948.pdf',
            'description': 'ä¿¡æ¯è®ºçš„å¥ åŸºä¹‹ä½œï¼Œå®šä¹‰äº†ç†µå’Œäº’ä¿¡æ¯ï¼Œå¥ å®šäº†æ•°å­—é€šä¿¡çš„ç†è®ºåŸºç¡€ï¼Œå¯¹ ML ä¸­çš„æŸå¤±å‡½æ•°ã€ä¿¡æ¯ç“¶é¢ˆç­‰æœ‰æ·±è¿œå½±å“ã€‚',
            'keywords': ['information theory', 'entropy', 'mutual information', 'channel capacity', 'coding theory']
        },
        {
            'title': 'Information Bottleneck Method',
            'authors': 'Tishby, Pereira, Biale',
            'year': '2000',
            'url': 'https://arxiv.org/abs/0001.2103',
            'description': 'ä¿¡æ¯ç“¶é¢ˆæ–¹æ³•ï¼Œç”¨äºç†è§£ç¥ç»ç½‘ç»œä¸­çš„è¡¨ç¤ºå­¦ä¹ ã€‚',
            'keywords': ['information bottleneck', 'representation learning', 'compression', 'mutual information', 'minimal sufficient statistic']
        }
    ]
}


class ClassicPaperFetcher:
    """ç»å…¸è®ºæ–‡è·å–å™¨"""

    def __init__(self, categories: List[str] = None):
        """
        åˆå§‹åŒ–

        Args:
            categories: è¦è·å–çš„ç±»åˆ«
        """
        self.categories = categories or list(CLASSIC_PAPERS.keys())

    def get_papers(self, limit: int = None) -> List[Dict]:
        """è·å–ç»å…¸è®ºæ–‡åˆ—è¡¨"""
        papers = []

        for category in self.categories:
            if category in CLASSIC_PAPERS:
                for paper in CLASSIC_PAPERS[category]:
                    papers.append({
                        **paper,
                        'category': category,
                        'source': 'classic',
                        'is_classic': True
                    })

        if limit:
            papers = papers[:limit]

        return papers

    def get_random_paper(self) -> Dict:
        """è·å–ä¸€ç¯‡éšæœºç»å…¸è®ºæ–‡"""
        import random
        all_papers = self.get_papers()
        return random.choice(all_papers) if all_papers else None

    def get_papers_by_keyword(self, keyword: str) -> List[Dict]:
        """æ ¹æ®å…³é”®è¯æœç´¢ç›¸å…³è®ºæ–‡"""
        keyword = keyword.lower()
        results = []

        for category, papers in CLASSIC_PAPERS.items():
            for paper in papers:
                # æœç´¢æ ‡é¢˜ã€æè¿°å’Œå…³é”®è¯
                if (keyword in paper['title'].lower() or
                    keyword in paper['description'].lower() or
                    any(keyword in kw.lower() for kw in paper.get('keywords', []))):
                    results.append({
                        **paper,
                        'category': category,
                        'source': 'classic'
                    })

        return results

    def format_keywords_analysis(self, paper: Dict) -> str:
        """æ ¼å¼åŒ–é¢†åŸŸå…³é”®è¯è§£æ"""
        keywords = paper.get('keywords', [])
        if not keywords:
            return ""

        analysis = f"\nğŸ”‘ **é¢†åŸŸå…³é”®è¯è§£æ**:\n"
        analysis += f"è¿™ç¯‡è®ºæ–‡å±äº **{paper['category']}** é¢†åŸŸï¼Œæ ¸å¿ƒæ¦‚å¿µåŒ…æ‹¬ï¼š\n\n"
        analysis += "```"
        for kw in keywords:
            analysis += f"â€¢ {kw}\n"
        analysis += "```\n\n"

        # æ·»åŠ ç›¸å…³é¢†åŸŸçš„äº¤å‰å‚è€ƒ
        related = self._find_related_categories(paper['category'], keywords)
        if related:
            analysis += f"ğŸ”— **ç›¸å…³é¢†åŸŸ**: {', '.join(related)}\n\n"

        return analysis

    def _find_related_categories(self, current_category: str, keywords: List[str]) -> List[str]:
        """æ‰¾å‡ºç›¸å…³çš„å…¶ä»–ç±»åˆ«"""
        category_relations = {
            'reinforcement_learning': ['llm', 'agents', 'alignment'],
            'alignment': ['llm', 'reinforcement_learning'],
            'ai4math': ['llm', 'reasoning'],
            'formal_verification': ['ai4math', 'reasoning'],
            'llm': ['alignment', 'reasoning']
        }

        return category_relations.get(current_category, [])


def format_classic_paper_card(paper: Dict) -> Dict:
    """æ ¼å¼åŒ–ä¸ºé£ä¹¦å¡ç‰‡å…ƒç´ """
    fetcher = ClassicPaperFetcher()

    title = f"ğŸ“– {paper['title']}"
    if paper.get('year'):
        title += f" ({paper['year']})"

    content = f"**{paper['title']}** ({paper.get('year', 'N/A')})\n\n"
    content += f"ğŸ‘¥ **ä½œè€…**: {paper['authors']}\n\n"
    content += f"ğŸ“ **ç®€ä»‹**: {paper['description']}\n\n"

    # æ·»åŠ å…³é”®è¯è§£æ
    keywords_analysis = fetcher.format_keywords_analysis(paper)
    content += keywords_analysis

    return {
        "tag": "div",
        "text": {
            "tag": "lark_md",
            "content": content
        }
    }


if __name__ == "__main__":
    fetcher = ClassicPaperFetcher(['reinforcement_learning', 'alignment'])
    papers = fetcher.get_papers()

    print("=" * 60)
    print("ğŸ“š ç»å…¸è®ºæ–‡")
    print("=" * 60)

    for i, paper in enumerate(papers, 1):
        print(f"\n{i}. {paper['title']} ({paper['year']})")
        print(f"   ä½œè€…: {paper['authors']}")
        print(f"   ç®€ä»‹: {paper['description']}")
        print(f"   å…³é”®è¯: {', '.join(paper.get('keywords', []))}")
