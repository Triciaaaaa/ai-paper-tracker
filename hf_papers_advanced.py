#!/usr/bin/env python3
"""
ğŸ¤– Hugging Face Daily Papers é«˜çº§æ¨é€æœºå™¨äºº
- è·å– HF Papers + å„å¤§å®éªŒå®¤åšå®¢
- AI æ™ºèƒ½æ‘˜è¦
- ç±»åˆ«è¿‡æ»¤
- é£ä¹¦æ¨é€
"""

import os
import sys
from datetime import datetime

from hf_paper_fetcher import HuggingFacePaperFetcher
from blog_fetcher import BlogFetcher
from ai_summarizer import AISummarizer, get_summarizer_from_env
from feishu_pusher import FeishuBotPusher, get_pusher_from_env
from classic_papers_extended import ClassicPaperFetcher, format_classic_paper_card


# ============ é…ç½®åŒº ============

# ç¯å¢ƒå˜é‡é…ç½®
DAYS_BACK = int(os.getenv('HF_DAYS_BACK', '7'))  # å‡å°‘åˆ° 7 å¤©
MAX_PAPERS = int(os.getenv('HF_MAX_PAPERS', '6'))  # å‡å°‘åˆ° 6 ç¯‡ï¼Œä¼˜ä¸­é€‰ä¼˜
MAX_BLOGS = int(os.getenv('HF_MAX_BLOGS', '3'))  # å‡å°‘åˆ° 3 ç¯‡ï¼Œåªä¿ç•™é«˜è´¨é‡çš„
USE_TRENDING = os.getenv('HF_USE_TRENDING', 'false').lower() == 'true'

# ç±»åˆ«è¿‡æ»¤ï¼ˆé€—å·åˆ†éš”ï¼‰ï¼Œé»˜è®¤åªçœ‹æ ¸å¿ƒé¢†åŸŸ
CATEGORY_FILTERS = os.getenv('HF_CATEGORIES', 'rl_verification,alignment,ai4math,auto_formalization')
CATEGORY_FILTERS = [c.strip() for c in CATEGORY_FILTERS.split(',') if c.strip()] if CATEGORY_FILTERS else None

# åšå®¢æºï¼ˆé»˜è®¤ä½¿ç”¨æœ‰æ´»è·ƒ RSS çš„æºï¼‰
# ä¼ä¸šåšå®¢: google_ai, deepmind, openai, microsoft_research, salesforce_ai, anthropic
# ä¸ªäººåšå®¢: lesswrong, jeremykun, colah, distill
# AI åª’ä½“: mit_tech_review
BLOG_SOURCES = os.getenv('HF_BLOG_SOURCES', 'google_ai,deepmind,openai,lesswrong,microsoft_research,salesforce_ai,mit_tech_review,jeremykun,colah,distill')
BLOG_SOURCES = [s.strip() for s in BLOG_SOURCES.split(',') if s.strip()]

# AI æ‘˜è¦é…ç½®ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
ENABLE_AI_SUMMARY = os.getenv('HF_ENABLE_AI_SUMMARY', 'true').lower() == 'true'
AI_PROVIDER = os.getenv('AI_PROVIDER', 'claude')  # é»˜è®¤ç”¨ Claude

# æ˜¯å¦åŒ…å«ç»å…¸è®ºæ–‡
INCLUDE_CLASSIC = os.getenv('HF_INCLUDE_CLASSIC', 'true').lower() == 'true'


# ============ ä¸»é€»è¾‘ ============

def format_datetime(date_str: str) -> str:
    """æ ¼å¼åŒ–æ—¥æœŸæ—¶é—´"""
    try:
        dt = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
        return dt.strftime('%Y-%m-%d %H:%M')
    except:
        return date_str[:19] if date_str else ''


def generate_trend_summary(summarizer, papers: list, blogs: list) -> str:
    """ç”Ÿæˆç ”ç©¶è¶‹åŠ¿æ€»ç»“"""
    try:
        # æ„å»ºå†…å®¹æ‘˜è¦
        content_parts = []

        if papers:
            content_parts.append("## è®ºæ–‡\n")
            for i, p in enumerate(papers[:5], 1):
                content_parts.append(f"{i}. {p['title']}\n")

        if blogs:
            content_parts.append("\n## åšå®¢\n")
            for i, b in enumerate(blogs[:3], 1):
                content_parts.append(f"{i}. {b['title']}\n")

        content = '\n'.join(content_parts)

        prompt = f"""åŸºäºä»¥ä¸‹ä»Šå¤©æ”¶é›†çš„ AI ç ”ç©¶è®ºæ–‡å’Œåšå®¢æ–‡ç« ï¼Œæ€»ç»“å½“å‰çš„ç ”ç©¶è¶‹åŠ¿ï¼š

{content}

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œ200-300 å­—ï¼Œé‡ç‚¹åˆ†æï¼š
1. ä¸»è¦ç ”ç©¶æ–¹å‘æœ‰å“ªäº›ï¼Ÿ
2. æœ‰å“ªäº›æ–°çš„æŠ€æœ¯è¶‹åŠ¿æˆ–æ–¹æ³•ï¼Ÿ
3. æ•´ä½“å‘ˆç°å‡ºä»€ä¹ˆå‘å±•æ€åŠ¿ï¼Ÿ

ç›´æ¥ç»™å‡ºæ€»ç»“ï¼Œä¸éœ€è¦å®¢å¥—è¯ã€‚"""

        # è°ƒç”¨ LLM ç”Ÿæˆè¶‹åŠ¿æ€»ç»“
        import os
        if os.getenv('OPENAI_BASE_URL') or summarizer.provider == 'openai':
            import openai
            base_url = os.getenv('OPENAI_BASE_URL', 'https://api.openai.com/v1')
            if not base_url.endswith('/v1'):
                base_url = base_url.rstrip('/') + '/v1'

            client = openai.OpenAI(api_key=summarizer.api_key, base_url=base_url)

            response = client.chat.completions.create(
                model=summarizer.model,
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ª AI ç ”ç©¶è¶‹åŠ¿åˆ†æå¸ˆï¼Œæ“…é•¿ä»å¤§é‡ç ”ç©¶å†…å®¹ä¸­æç‚¼å…³é”®è¶‹åŠ¿ã€‚"
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                max_tokens=800,
                temperature=0.7
            )

            summary = response.choices[0].message.content.strip()
            print(f"âœ… è¶‹åŠ¿æ€»ç»“ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(summary)} å­—ç¬¦")
            return summary

    except Exception as e:
        print(f"âš ï¸  è¶‹åŠ¿æ€»ç»“ç”Ÿæˆå¤±è´¥: {e}")
        return None


def main():
    """ä¸»å‡½æ•°"""

    print("=" * 60)
    print("ğŸ¤– HF Daily Papers + åšå®¢ é«˜çº§æ¨é€æœºå™¨äºº")
    print("=" * 60)

    # æ£€æŸ¥æ¨é€é…ç½®
    pusher = get_pusher_from_env()
    if not pusher:
        print("âŒ æœªé…ç½®é£ä¹¦ Webhook")
        print("\nè¯·è®¾ç½®ç¯å¢ƒå˜é‡:")
        print("  export FEISHU_WEBHOOK_URL='ä½ çš„webhookåœ°å€'")
        return 1

    print(f"ğŸ“± æ¨é€æ–¹å¼: é£ä¹¦ç¾¤èŠæœºå™¨äºº")
    print(f"ğŸ“… è·å–å¤©æ•°: {DAYS_BACK} å¤©")
    print(f"ğŸ“Š æœ€å¤šè®ºæ–‡: {MAX_PAPERS} ç¯‡")
    print(f"ğŸ“° åšå®¢æº: {', '.join(BLOG_SOURCES)}")
    print(f"ğŸ·ï¸  ç±»åˆ«è¿‡æ»¤: {', '.join(CATEGORY_FILTERS) if CATEGORY_FILTERS else 'æ— '}")
    print(f"ğŸ¤– AI æ‘˜è¦: {AI_PROVIDER if ENABLE_AI_SUMMARY else 'ç¦ç”¨'}")
    print(f"ğŸ“š ç»å…¸è®ºæ–‡: {'åŒ…å«' if INCLUDE_CLASSIC else 'ä¸åŒ…å«'}")

    # åˆå§‹åŒ– AI æ‘˜è¦å™¨
    summarizer = None
    if ENABLE_AI_SUMMARY:
        summarizer = get_summarizer_from_env()
        if not summarizer.api_key:
            print(f"âš ï¸  æœªé…ç½® {AI_PROVIDER} API keyï¼Œè·³è¿‡ AI æ‘˜è¦")
            summarizer = None

    # ========== 1. è·å– HF è®ºæ–‡ ==========
    print("\n" + "=" * 60)
    print("ğŸ“š è·å– Hugging Face è®ºæ–‡")
    print("=" * 60)

    fetcher = HuggingFacePaperFetcher(
        days_back=DAYS_BACK,
        max_papers=MAX_PAPERS,
        category_filters=CATEGORY_FILTERS
    )

    if USE_TRENDING:
        papers = fetcher.fetch_trending_papers()
    else:
        papers = fetcher.fetch_recent_papers()

    if not papers:
        print("âš ï¸  æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡")
        papers = []

    # AI æ‘˜è¦å¤„ç†
    if summarizer:
        print(f"\nğŸ¤– ç”Ÿæˆ AI è¯¦ç»†è§£è¯»...")
        for i, paper in enumerate(papers):
            print(f"  [{i+1}/{len(papers)}] {paper['title'][:40]}...")

            # æ€»æ˜¯è°ƒç”¨ LLM ç”Ÿæˆè¯¦ç»†è§£è¯»ï¼ˆä¸å†ä½¿ç”¨ç®€çŸ­çš„ HF æ‘˜è¦ï¼‰
            summary = summarizer.summarize_paper(paper, use_hf_summary=False)
            if summary:
                paper['ai_enhanced_summary'] = summary
            else:
                # å¦‚æœ LLM å¤±è´¥ï¼Œä½¿ç”¨ HF æ‘˜è¦ä½œä¸ºå¤‡é€‰
                if paper.get('ai_summary'):
                    paper['ai_enhanced_summary'] = f"ğŸ“Œ **HF AI**: {paper['ai_summary']}"
                else:
                    paper['ai_enhanced_summary'] = None

    # ========== 2. è·å–åšå®¢æ–‡ç«  ==========
    print("\n" + "=" * 60)
    print("ğŸ“° è·å–åšå®¢æ–‡ç« ")
    print("=" * 60)

    blog_fetcher = BlogFetcher(days_back=DAYS_BACK * 2, max_articles=MAX_BLOGS)
    blogs = blog_fetcher.fetch_blogs(BLOG_SOURCES, fetch_full_content=summarizer is not None)

    # è¿‡æ»¤ï¼šåªä¿ç•™èƒ½è·å–åˆ°å…¨æ–‡çš„åšå®¢
    if blogs:
        original_count = len(blogs)
        blogs = [b for b in blogs if b.get('full_content')]
        filtered_count = original_count - len(blogs)
        if filtered_count > 0:
            print(f"  ğŸ“‹ è¿‡æ»¤æ‰ {filtered_count} ç¯‡æ— æ³•è·å–å…¨æ–‡çš„åšå®¢")

    # é™åˆ¶æœ€ç»ˆæ•°é‡ï¼ˆä¼˜ä¸­é€‰ä¼˜ï¼Œåªä¿ç•™æœ€æ–°çš„å‡ ç¯‡ï¼‰
    if blogs and len(blogs) > MAX_BLOGS:
        print(f"  ğŸ¯ ä» {len(blogs)} ç¯‡ä¸­ç²¾é€‰æœ€æ–°çš„ {MAX_BLOGS} ç¯‡")
        blogs = blogs[:MAX_BLOGS]

    # ä¸ºåšå®¢ç”Ÿæˆç®€çŸ­ AI è§£è¯»
    if summarizer and blogs:
        print(f"\nğŸ¤– ç”Ÿæˆåšå®¢ AI è§£è¯»...")
        for i, blog in enumerate(blogs):
            print(f"  [{i+1}/{len(blogs)}] {blog['title'][:40]}...")
            summary = summarizer.summarize_blog(blog)
            if summary:
                blog['ai_summary'] = summary
            else:
                blog['ai_summary'] = None

    if not blogs:
        print("âš ï¸  æœªæ‰¾åˆ°åšå®¢æ–‡ç« ")
        blogs = []

    # ========== 3. è·å–ç»å…¸è®ºæ–‡ ==========
    classic_paper = None
    if INCLUDE_CLASSIC:
        print("\n" + "=" * 60)
        print("ğŸ“– è·å–ç»å…¸è®ºæ–‡æ¨è")
        print("=" * 60)

        classic_fetcher = ClassicPaperFetcher(
            categories=['reinforcement_learning', 'alignment', 'ai4math', 'formal_verification', 'llm', 'information_theory']
        )
        classic_paper = classic_fetcher.get_random_paper()
        if classic_paper:
            # æ·»åŠ å…³é”®è¯è§£æ
            keywords_analysis = classic_fetcher.format_keywords_analysis(classic_paper)
            print(f"âœ… ä»Šæ—¥æ¨è: {classic_paper['title']}")
            if keywords_analysis:
                print(f"   å…³é”®è¯: {', '.join(classic_paper.get('keywords', [])[:5])}")
        else:
            print("âš ï¸  æœªæ‰¾åˆ°ç»å…¸è®ºæ–‡")

    # ========== 4. ç”Ÿæˆç ”ç©¶è¶‹åŠ¿æ€»ç»“ ==========
    trend_summary = None
    if summarizer and (papers or blogs):
        print("\n" + "=" * 60)
        print("ğŸ“Š ç”Ÿæˆç ”ç©¶è¶‹åŠ¿æ€»ç»“")
        print("=" * 60)

        trend_summary = generate_trend_summary(summarizer, papers, blogs)

    # ========== 5. æ„å»ºæ¨é€å†…å®¹ ==========
    print("\n" + "=" * 60)
    print("ğŸ“ æ„å»ºæ¨é€æ¶ˆæ¯")
    print("=" * 60)

    card = build_enhanced_card(papers, blogs, classic_paper, trend_summary)
    print(f"âœ… æ„å»ºå®Œæˆ")

    # ========== 5. å‘é€æ¨é€ ==========
    print("\n" + "=" * 60)
    print("ğŸ“¤ å‘é€åˆ°é£ä¹¦")
    print("=" * 60)

    success = pusher.send_interactive_card(card)

    if success:
        print(f"\nâœ… æ¨é€æˆåŠŸï¼")
        print(f"\nğŸ“Š ç»Ÿè®¡:")
        print(f"  â€¢ è®ºæ–‡æ•°: {len(papers)}")
        print(f"  â€¢ åšå®¢æ•°: {len(blogs)}")
        print(f"  â€¢ AI æ‘˜è¦: {'æ˜¯' if ENABLE_AI_SUMMARY else 'å¦'}")
    else:
        print(f"\nâŒ æ¨é€å¤±è´¥")
        return 1

    print("\n" + "=" * 60)
    return 0


def build_enhanced_card(papers: list, blogs: list, classic_paper: dict = None, trend_summary: str = None) -> dict:
    """æ„å»ºå¢å¼ºç‰ˆé£ä¹¦å¡ç‰‡"""

    elements = []

    # ========== æ ‡é¢˜åŒº ==========
    now = datetime.now().strftime('%Y-%m-%d %H:%M')

    elements.append({
        "tag": "div",
        "text": {
            "tag": "lark_md",
            "content": f"**ğŸ“Š è®ºæ–‡: {len(papers)} ç¯‡ | åšå®¢: {len(blogs)} ç¯‡**\n**â° {now}**"
        }
    })

    elements.append({"tag": "hr"})

    # ========== è¶‹åŠ¿æ€»ç»“åŒº ==========
    if trend_summary:
        elements.append({
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": f"**ğŸ“ˆ ä»Šæ—¥ç ”ç©¶è¶‹åŠ¿**\n\n{trend_summary}"
            }
        })

        elements.append({"tag": "hr"})

    # ========== ç»å…¸è®ºæ–‡åŒºï¼ˆæ”¾åœ¨å‰é¢ï¼‰ ==========
    if classic_paper:
        elements.append({
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": "**ğŸ“– æ¯æ—¥ç»å…¸è®ºæ–‡æ¨è**"
            }
        })

        # ç»å…¸è®ºæ–‡å†…å®¹ï¼ˆåŒ…å«å…³é”®è¯è§£æï¼‰
        classic_content = f"**{classic_paper['title']}** ({classic_paper.get('year', 'N/A')})\n\n"
        classic_content += f"ğŸ‘¥ **ä½œè€…**: {classic_paper['authors']}\n\n"
        classic_content += f"ğŸ“ **ç®€ä»‹**: {classic_paper['description']}\n\n"

        # æ·»åŠ å…³é”®è¯è§£æ
        keywords = classic_paper.get('keywords', [])
        if keywords:
            classic_content += f"ğŸ”‘ **æ ¸å¿ƒæ¦‚å¿µ**: {', '.join(keywords[:5])}"
            if len(keywords) > 5:
                classic_content += f" ç­‰ {len(keywords)} ä¸ªå…³é”®è¯"
            classic_content += "\n\n"

        elements.append({
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": classic_content
            }
        })

        elements.append({
            "tag": "action",
            "actions": [
                {
                    "tag": "button",
                    "text": {"tag": "plain_text", "content": "æŸ¥çœ‹è®ºæ–‡"},
                    "type": "default",
                    "url": classic_paper['url']
                }
            ]
        })

        elements.append({"tag": "hr"})

    # ========== è®ºæ–‡åŒº ==========
    if papers:
        elements.append({
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": "**ğŸ“š Hugging Face è®ºæ–‡**"
            }
        })

        for i, paper in enumerate(papers, 1):
            # æ ‡é¢˜
            title_text = f"**{i}. {paper['title']}**"
            if paper.get('categories'):
                tags = ' '.join([f"`{cat}`" for cat in paper['categories'][:3]])
                title_text += f"\nğŸ·ï¸  {tags}"

            elements.append({
                "tag": "div",
                "text": {
                    "tag": "lark_md",
                    "content": title_text
                }
            })

            # ä½œè€…å’Œå‘å¸ƒæ—¶é—´
            meta_lines = []
            if paper.get('author_str'):
                meta_lines.append(f"ğŸ‘¥ {paper['author_str']}")
            if paper.get('published'):
                meta_lines.append(f"ğŸ“… {format_datetime(paper['published'])}")

            if meta_lines:
                elements.append({
                    "tag": "div",
                    "text": {
                        "tag": "lark_md",
                        "content": ' | '.join(meta_lines)
                    }
                })

            # æ‘˜è¦ï¼ˆAI è§£è¯»ä¼˜å…ˆï¼‰
            if paper.get('ai_enhanced_summary'):
                # AI è§£è¯»ï¼Œæ˜¾ç¤ºæ›´å¤šå†…å®¹
                summary = paper['ai_enhanced_summary']
                # é£ä¹¦å¡ç‰‡å†…å®¹é™åˆ¶ï¼Œé€‚å½“æˆªæ–­
                if len(summary) > 1500:
                    summary = summary[:1500] + '\n\n... (å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­)'
                elements.append({
                    "tag": "div",
                    "text": {
                        "tag": "lark_md",
                        "content": summary
                    }
                })
            elif paper.get('summary'):
                # åŸå§‹æ‘˜è¦
                summary = paper['summary']
                if len(summary) > 300:
                    summary = summary[:300] + '...'
                elements.append({
                    "tag": "div",
                    "text": {
                        "tag": "lark_md",
                        "content": f"ğŸ“ {summary}"
                    }
                })

            # é“¾æ¥æŒ‰é’®
            actions = [
                {
                    "tag": "button",
                    "text": {"tag": "plain_text", "content": "æŸ¥çœ‹è®ºæ–‡"},
                    "type": "default",
                    "url": paper['paper_url']
                }
            ]

            if paper.get('pdf_url'):
                actions.append({
                    "tag": "button",
                    "text": {"tag": "plain_text", "content": "ä¸‹è½½ PDF"},
                    "type": "primary",
                    "url": paper['pdf_url']
                })

            if paper.get('project_page'):
                actions.append({
                    "tag": "button",
                    "text": {"tag": "plain_text", "content": "é¡¹ç›®ä¸»é¡µ"},
                    "type": "default",
                    "url": paper['project_page']
                })

            elements.append({
                "tag": "action",
                "actions": actions
            })

            # åˆ†éš”çº¿
            if i < len(papers):
                elements.append({"tag": "hr"})

    # ========== åšå®¢åŒº ==========
    if blogs:
        elements.append({"tag": "hr"})
        elements.append({
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": "**ğŸ“° å®éªŒå®¤åšå®¢**"
            }
        })

        for blog in blogs[:5]:  # æœ€å¤šæ˜¾ç¤º 5 ç¯‡
            # æ ‡é¢˜å’Œå…ƒä¿¡æ¯
            elements.append({
                "tag": "div",
                "text": {
                    "tag": "lark_md",
                    "content": f"**â€¢ {blog['title']}**\nğŸ¢ {blog['source']} | ğŸ“… {format_datetime(blog['published'])}"
                }
            })

            # é“¾æ¥æŒ‰é’®
            elements.append({
                "tag": "action",
                "actions": [
                    {
                        "tag": "button",
                        "text": {"tag": "plain_text", "content": "é˜…è¯»æ–‡ç« "},
                        "type": "default",
                        "url": blog['link']
                    }
                ]
            })

            # åˆ†éš”çº¿
            if blogs.index(blog) < min(len(blogs), 5) - 1:
                elements.append({"tag": "hr"})

    # ========== æ„å»ºå¡ç‰‡ ==========
    card = {
        "config": {
            "wide_screen_mode": True
        },
        "header": {
            "title": {
                "tag": "plain_text",
                "content": "ğŸ¤– AI Research Daily"
            },
            "template": "blue"
        },
        "elements": elements
    }

    return card


if __name__ == "__main__":
    sys.exit(main())
