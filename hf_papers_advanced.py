#!/usr/bin/env python3
"""
ğŸ¤– Hugging Face Daily Papers é«˜çº§æ¨é€æœºå™¨äºº
- è·å– HF Papers + å„å¤§å®éªŒå®¤åšå®¢
- AI æ™ºèƒ½æ‘˜è¦
- ç±»åˆ«è¿‡æ»¤
- é£ä¹¦æ¨é€
"""

import os
import sys
import json
from datetime import datetime

from hf_paper_fetcher import HuggingFacePaperFetcher
from blog_fetcher import BlogFetcher
from ai_summarizer import AISummarizer, get_summarizer_from_env
from feishu_pusher import FeishuBotPusher, get_pusher_from_env
from classic_papers_extended import ClassicPaperFetcher, format_classic_paper_card

# æ¨é€å†å²è®°å½•æ–‡ä»¶ï¼Œç”¨äºå»é‡
PUSH_HISTORY_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), '.push_history.json')


# ============ é…ç½®åŒº ============

# ç¯å¢ƒå˜é‡é…ç½®
DAYS_BACK = int(os.getenv('HF_DAYS_BACK', '7'))  # å‡å°‘åˆ° 7 å¤©
MAX_PAPERS = int(os.getenv('HF_MAX_PAPERS', '6'))  # å‡å°‘åˆ° 6 ç¯‡ï¼Œä¼˜ä¸­é€‰ä¼˜
MAX_BLOGS = int(os.getenv('HF_MAX_BLOGS', '3'))  # å‡å°‘åˆ° 3 ç¯‡ï¼Œåªä¿ç•™é«˜è´¨é‡çš„
USE_TRENDING = os.getenv('HF_USE_TRENDING', 'false').lower() == 'true'

# ç±»åˆ«è¿‡æ»¤ï¼ˆé€—å·åˆ†éš”ï¼‰ï¼Œé»˜è®¤å…³æ³¨æ•°æ®å·¥ç¨‹å’Œ VLM ç›¸å…³é¢†åŸŸ
CATEGORY_FILTERS = os.getenv('HF_CATEGORIES', 'vlm_data_strategy,data_engineering,training_data_strategy,data_methodology,vision_language,alignment,reasoning')
CATEGORY_FILTERS = [c.strip() for c in CATEGORY_FILTERS.split(',') if c.strip()] if CATEGORY_FILTERS else None

# åšå®¢æºï¼ˆé»˜è®¤ä½¿ç”¨æœ‰æ´»è·ƒ RSS çš„æºï¼‰
# ä¼ä¸šåšå®¢: google_ai, deepmind, openai, microsoft_research, salesforce_ai, anthropic
# ä¸ªäººåšå®¢: lesswrong, jeremykun, colah, distill
# AI åª’ä½“: mit_tech_review
BLOG_SOURCES = os.getenv('HF_BLOG_SOURCES', 'hn_ai,karpathy,simon_willison,tim_dettmers,chip_huyen,openai,anthropic,deepmind,google_ai,meta_ai,microsoft_research,huggingface,the_gradient,lesswrong,mit_tech_review,bair,nvidia')
BLOG_SOURCES = [s.strip() for s in BLOG_SOURCES.split(',') if s.strip()]

# AI æ‘˜è¦é…ç½®ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
ENABLE_AI_SUMMARY = os.getenv('HF_ENABLE_AI_SUMMARY', 'true').lower() == 'true'
AI_PROVIDER = os.getenv('AI_PROVIDER', 'claude')  # é»˜è®¤ç”¨ Claude

# æ˜¯å¦åŒ…å«ç»å…¸è®ºæ–‡
INCLUDE_CLASSIC = os.getenv('HF_INCLUDE_CLASSIC', 'true').lower() == 'true'

# æ˜¯å¦åŒ…å« Twitter æ¨æ–‡
ENABLE_TWITTER = os.getenv('HF_ENABLE_TWITTER', 'false').lower() == 'true'


# ============ ä¸»é€»è¾‘ ============

def format_datetime(date_str: str) -> str:
    """æ ¼å¼åŒ–æ—¥æœŸæ—¶é—´"""
    try:
        dt = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
        return dt.strftime('%Y-%m-%d %H:%M')
    except:
        return date_str[:19] if date_str else ''


def generate_trend_summary(summarizer, papers: list, blogs: list, prev_summary: str = '') -> str:
    """ç”Ÿæˆç ”ç©¶è¶‹åŠ¿æ€»ç»“ï¼Œå¯å‚è€ƒæ˜¨æ—¥å†…å®¹ä½“ç°å»¶ç»­æ€§"""
    try:
        content_parts = []

        if papers:
            content_parts.append("ä»Šæ—¥è®ºæ–‡:")
            for i, p in enumerate(papers[:5], 1):
                content_parts.append(f"  {i}. {p['title']}")

        if blogs:
            content_parts.append("ä»Šæ—¥åšå®¢:")
            for i, b in enumerate(blogs[:3], 1):
                content_parts.append(f"  {i}. {b['title']}")

        content = '\n'.join(content_parts)

        prev_section = ""
        if prev_summary:
            prev_section = f"""
æ˜¨æ—¥æ¨é€æ‘˜è¦ï¼š
{prev_summary}

"""

        prompt = f"""ç”¨ä¸­æ–‡æ€»ç»“ä»Šæ—¥ AI ç ”ç©¶åŠ¨æ€ï¼Œ100-150 å­—ï¼š
{prev_section}
{content}

è¦æ±‚ï¼š
1. æ¦‚æ‹¬ä»Šæ—¥ä¸»è¦æ–¹å‘ï¼ˆ1-2 å¥ï¼‰
2. {'ä¸æ˜¨æ—¥å¯¹æ¯”ï¼Œæœ‰ä»€ä¹ˆæ–°å˜åŒ–æˆ–å»¶ç»­ï¼ˆ1 å¥ï¼‰' if prev_summary else 'å€¼å¾—å…³æ³¨çš„è¶‹åŠ¿ï¼ˆ1 å¥ï¼‰'}
3. ä¸€å¥è¯å±•æœ›

ç®€æ´ç›´æ¥ã€‚"""

        # è°ƒç”¨ LLM ç”Ÿæˆè¶‹åŠ¿æ€»ç»“
        import os
        if os.getenv('OPENAI_BASE_URL') or summarizer.provider == 'openai':
            import openai
            base_url = os.getenv('OPENAI_BASE_URL', 'https://api.openai.com/v1')
            if not base_url.endswith('/v1'):
                base_url = base_url.rstrip('/') + '/v1'

            client = openai.OpenAI(api_key=summarizer.api_key, base_url=base_url)

            response = client.chat.completions.create(
                model=summarizer.model,
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ª AI ç ”ç©¶è¶‹åŠ¿åˆ†æå¸ˆï¼Œæ“…é•¿ä»å¤§é‡ç ”ç©¶å†…å®¹ä¸­æç‚¼å…³é”®è¶‹åŠ¿ã€‚"
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                max_tokens=800,
                temperature=0.7
            )

            summary = response.choices[0].message.content.strip()
            print(f"âœ… è¶‹åŠ¿æ€»ç»“ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(summary)} å­—ç¬¦")
            return summary

    except Exception as e:
        print(f"âš ï¸  è¶‹åŠ¿æ€»ç»“ç”Ÿæˆå¤±è´¥: {e}")
        return None


def load_push_history() -> dict:
    """åŠ è½½æ¨é€å†å²ï¼Œç”¨äºå»é‡å’Œå»¶ç»­æ€§"""
    try:
        if os.path.exists(PUSH_HISTORY_FILE):
            with open(PUSH_HISTORY_FILE, 'r') as f:
                history = json.load(f)
            from datetime import timedelta
            cutoff = (datetime.now() - timedelta(days=14)).isoformat()
            history['papers'] = {k: v for k, v in history.get('papers', {}).items() if v > cutoff}
            history['blogs'] = {k: v for k, v in history.get('blogs', {}).items() if v > cutoff}
            # prev_summary ä¿ç•™ä¸Šæ¬¡æ¨é€çš„ç®€è¦å†…å®¹ï¼Œç”¨äºå»¶ç»­æ€§
            history.setdefault('prev_summary', '')
            return history
    except Exception as e:
        print(f"  âš ï¸  åŠ è½½æ¨é€å†å²å¤±è´¥: {e}")
    return {'papers': {}, 'blogs': {}, 'prev_summary': ''}


def save_push_history(history: dict):
    """ä¿å­˜æ¨é€å†å²"""
    try:
        with open(PUSH_HISTORY_FILE, 'w') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"  âš ï¸  ä¿å­˜æ¨é€å†å²å¤±è´¥: {e}")


def dedup_items(items: list, history_key: dict, id_field: str) -> list:
    """æ ¹æ®å†å²è®°å½•å»é‡ï¼Œè¿”å›æœªæ¨é€è¿‡çš„æ¡ç›®"""
    new_items = []
    for item in items:
        item_id = item.get(id_field, item.get('title', ''))
        if item_id and item_id not in history_key:
            new_items.append(item)
    return new_items


def main():
    """ä¸»å‡½æ•°"""

    print("=" * 60)
    print("ğŸ¤– HF Daily Papers + åšå®¢ é«˜çº§æ¨é€æœºå™¨äºº")
    print("=" * 60)

    # æ£€æŸ¥æ¨é€é…ç½®
    pusher = get_pusher_from_env()
    if not pusher:
        print("âŒ æœªé…ç½®é£ä¹¦ Webhook")
        print("\nè¯·è®¾ç½®ç¯å¢ƒå˜é‡:")
        print("  export FEISHU_WEBHOOK_URL='ä½ çš„webhookåœ°å€'")
        return 1

    print(f"ğŸ“± æ¨é€æ–¹å¼: é£ä¹¦ç¾¤èŠæœºå™¨äºº")
    print(f"ğŸ“… è·å–å¤©æ•°: {DAYS_BACK} å¤©")
    print(f"ğŸ“Š æœ€å¤šè®ºæ–‡: {MAX_PAPERS} ç¯‡")
    print(f"ğŸ“° åšå®¢æº: {', '.join(BLOG_SOURCES)}")
    print(f"ğŸ·ï¸  ç±»åˆ«è¿‡æ»¤: {', '.join(CATEGORY_FILTERS) if CATEGORY_FILTERS else 'æ— '}")
    print(f"ğŸ¤– AI æ‘˜è¦: {AI_PROVIDER if ENABLE_AI_SUMMARY else 'ç¦ç”¨'}")
    print(f"ğŸ“š ç»å…¸è®ºæ–‡: {'åŒ…å«' if INCLUDE_CLASSIC else 'ä¸åŒ…å«'}")

    # åˆå§‹åŒ– AI æ‘˜è¦å™¨
    summarizer = None
    if ENABLE_AI_SUMMARY:
        summarizer = get_summarizer_from_env()
        if not summarizer.api_key:
            print(f"âš ï¸  æœªé…ç½® {AI_PROVIDER} API keyï¼Œè·³è¿‡ AI æ‘˜è¦")
            summarizer = None

    # ========== 1. è·å– HF è®ºæ–‡ ==========
    print("\n" + "=" * 60)
    print("ğŸ“š è·å– Hugging Face è®ºæ–‡")
    print("=" * 60)

    fetcher = HuggingFacePaperFetcher(
        days_back=DAYS_BACK,
        max_papers=MAX_PAPERS,
        category_filters=CATEGORY_FILTERS
    )

    if USE_TRENDING:
        papers = fetcher.fetch_trending_papers()
    else:
        papers = fetcher.fetch_recent_papers()

    if not papers:
        print("âš ï¸  æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡")
        papers = []

    # å»é‡ï¼šæ’é™¤å·²æ¨é€è¿‡çš„è®ºæ–‡
    push_history = load_push_history()
    if papers:
        before = len(papers)
        papers = dedup_items(papers, push_history['papers'], 'paper_url')
        if before != len(papers):
            print(f"  ğŸ”„ å»é‡: {before} -> {len(papers)} ç¯‡ï¼ˆæ’é™¤ {before - len(papers)} ç¯‡å·²æ¨é€ï¼‰")

    # AI æ‘˜è¦å¤„ç†ï¼ˆä¼ å…¥æ˜¨æ—¥ä¸Šä¸‹æ–‡ï¼Œä½“ç°å»¶ç»­æ€§ï¼‰
    prev_context = push_history.get('prev_summary', '')
    if summarizer:
        print(f"\nğŸ¤– ç”Ÿæˆ AI è§£è¯»...")
        if prev_context:
            print(f"  ğŸ“ å·²åŠ è½½æ˜¨æ—¥æ¨é€ä¸Šä¸‹æ–‡ï¼ˆ{len(prev_context)} å­—ï¼‰")
        for i, paper in enumerate(papers):
            print(f"  [{i+1}/{len(papers)}] {paper['title'][:40]}...")

            summary = summarizer.summarize_paper(paper, use_hf_summary=False, prev_context=prev_context)
            if summary:
                paper['ai_enhanced_summary'] = summary
            else:
                if paper.get('ai_summary'):
                    paper['ai_enhanced_summary'] = f"ğŸ“Œ **HF AI**: {paper['ai_summary']}"
                else:
                    paper['ai_enhanced_summary'] = None

    # ========== 2. è·å–åšå®¢æ–‡ç«  ==========
    print("\n" + "=" * 60)
    print("ğŸ“° è·å–åšå®¢æ–‡ç« ")
    print("=" * 60)

    blog_fetcher = BlogFetcher(days_back=DAYS_BACK * 2, max_articles=MAX_BLOGS)
    blogs = blog_fetcher.fetch_blogs(BLOG_SOURCES, fetch_full_content=summarizer is not None)

    # è¿‡æ»¤ï¼šåªä¿ç•™èƒ½è·å–åˆ°å…¨æ–‡çš„åšå®¢
    if blogs:
        original_count = len(blogs)
        blogs = [b for b in blogs if b.get('full_content')]
        filtered_count = original_count - len(blogs)
        if filtered_count > 0:
            print(f"  ğŸ“‹ è¿‡æ»¤æ‰ {filtered_count} ç¯‡æ— æ³•è·å–å…¨æ–‡çš„åšå®¢")

    # é™åˆ¶æœ€ç»ˆæ•°é‡ï¼ˆä¼˜ä¸­é€‰ä¼˜ï¼Œåªä¿ç•™æœ€æ–°çš„å‡ ç¯‡ï¼‰
    if blogs and len(blogs) > MAX_BLOGS:
        print(f"  ğŸ¯ ä» {len(blogs)} ç¯‡ä¸­ç²¾é€‰æœ€æ–°çš„ {MAX_BLOGS} ç¯‡")
        blogs = blogs[:MAX_BLOGS]

    # ä¸ºåšå®¢ç”Ÿæˆç®€çŸ­ AI è§£è¯»
    if summarizer and blogs:
        print(f"\nğŸ¤– ç”Ÿæˆåšå®¢ AI è§£è¯»...")
        for i, blog in enumerate(blogs):
            print(f"  [{i+1}/{len(blogs)}] {blog['title'][:40]}...")
            summary = summarizer.summarize_blog(blog)
            if summary:
                blog['ai_summary'] = summary
            else:
                blog['ai_summary'] = None

    # å»é‡ï¼šæ’é™¤å·²æ¨é€è¿‡çš„åšå®¢
    if blogs:
        before = len(blogs)
        blogs = dedup_items(blogs, push_history['blogs'], 'link')
        if before != len(blogs):
            print(f"  ğŸ”„ å»é‡: {before} -> {len(blogs)} ç¯‡ï¼ˆæ’é™¤ {before - len(blogs)} ç¯‡å·²æ¨é€ï¼‰")

    if not blogs:
        print("âš ï¸  æœªæ‰¾åˆ°åšå®¢æ–‡ç« ")
        blogs = []

    # ========== 2.5 è·å– Twitter æ¨æ–‡ï¼ˆå¯é€‰ï¼‰ ==========
    tweets = []
    if ENABLE_TWITTER:
        print("\n" + "=" * 60)
        print("ğŸ¦ è·å– AI ç ”ç©¶è€…æ¨æ–‡")
        print("=" * 60)

        try:
            from twitter_fetcher import TwitterFetcher
            tweet_fetcher = TwitterFetcher(max_tweets=3)
            tweets = tweet_fetcher.fetch_tweets()
            if tweets:
                print(f"âœ… è·å–åˆ° {len(tweets)} æ¡æ¨æ–‡")
            else:
                print("âš ï¸  æœªè·å–åˆ°æ¨æ–‡")
        except ImportError:
            print("âš ï¸  twitter_fetcher æ¨¡å—æœªå®‰è£…ï¼Œè·³è¿‡æ¨æ–‡è·å–")
        except Exception as e:
            print(f"âš ï¸  æ¨æ–‡è·å–å¤±è´¥: {e}")

    # ========== 3. è·å–ç»å…¸è®ºæ–‡ ==========
    classic_paper = None
    if INCLUDE_CLASSIC:
        print("\n" + "=" * 60)
        print("ğŸ“– è·å–ç»å…¸è®ºæ–‡æ¨è")
        print("=" * 60)

        classic_fetcher = ClassicPaperFetcher(
            categories=['reinforcement_learning', 'alignment', 'ai4math', 'formal_verification', 'llm', 'information_theory']
        )
        classic_paper = classic_fetcher.get_random_paper()
        if classic_paper:
            # æ·»åŠ å…³é”®è¯è§£æ
            keywords_analysis = classic_fetcher.format_keywords_analysis(classic_paper)
            print(f"âœ… ä»Šæ—¥æ¨è: {classic_paper['title']}")
            if keywords_analysis:
                print(f"   å…³é”®è¯: {', '.join(classic_paper.get('keywords', [])[:5])}")
        else:
            print("âš ï¸  æœªæ‰¾åˆ°ç»å…¸è®ºæ–‡")

    # ========== 4. ç”Ÿæˆç ”ç©¶è¶‹åŠ¿æ€»ç»“ ==========
    trend_summary = None
    if summarizer and (papers or blogs):
        print("\n" + "=" * 60)
        print("ğŸ“Š ç”Ÿæˆç ”ç©¶è¶‹åŠ¿æ€»ç»“")
        print("=" * 60)

        trend_summary = generate_trend_summary(summarizer, papers, blogs, prev_context)

    # ========== 5. æ„å»ºæ¨é€å†…å®¹ ==========
    print("\n" + "=" * 60)
    print("ğŸ“ æ„å»ºæ¨é€æ¶ˆæ¯")
    print("=" * 60)

    card = build_enhanced_card(papers, blogs, classic_paper, trend_summary, tweets)
    print(f"âœ… æ„å»ºå®Œæˆ")

    # ========== 5. å‘é€æ¨é€ ==========
    print("\n" + "=" * 60)
    print("ğŸ“¤ å‘é€åˆ°é£ä¹¦")
    print("=" * 60)

    success = pusher.send_interactive_card(card)

    if success:
        print(f"\nâœ… æ¨é€æˆåŠŸï¼")
        print(f"\nğŸ“Š ç»Ÿè®¡:")
        print(f"  â€¢ è®ºæ–‡æ•°: {len(papers)}")
        print(f"  â€¢ åšå®¢æ•°: {len(blogs)}")
        print(f"  â€¢ AI æ‘˜è¦: {'æ˜¯' if ENABLE_AI_SUMMARY else 'å¦'}")

        # è®°å½•å·²æ¨é€å†…å®¹ï¼Œä¸‹æ¬¡å»é‡+å»¶ç»­æ€§ç”¨
        now = datetime.now().isoformat()
        for p in papers:
            push_history['papers'][p.get('paper_url', p['title'])] = now
        for b in blogs:
            push_history['blogs'][b.get('link', b['title'])] = now

        # ä¿å­˜ä»Šæ—¥æ‘˜è¦ä¾›æ˜å¤©å‚è€ƒï¼ˆè®ºæ–‡æ ‡é¢˜åˆ—è¡¨ + è¶‹åŠ¿æ€»ç»“ï¼‰
        today_titles = [p['title'] for p in papers[:5]]
        today_blog_titles = [b['title'] for b in blogs[:3]]
        prev_for_tomorrow = f"è®ºæ–‡: {'; '.join(today_titles)}"
        if today_blog_titles:
            prev_for_tomorrow += f"\nåšå®¢: {'; '.join(today_blog_titles)}"
        if trend_summary:
            prev_for_tomorrow += f"\nè¶‹åŠ¿: {trend_summary[:200]}"
        push_history['prev_summary'] = prev_for_tomorrow

        save_push_history(push_history)
        print(f"  â€¢ æ¨é€å†å²å·²æ›´æ–°ï¼ˆå«æ˜æ—¥å»¶ç»­ä¸Šä¸‹æ–‡ï¼‰")
    else:
        print(f"\nâŒ æ¨é€å¤±è´¥")
        return 1

    print("\n" + "=" * 60)
    return 0


def build_enhanced_card(papers: list, blogs: list, classic_paper: dict = None, trend_summary: str = None, tweets: list = None) -> dict:
    """æ„å»ºå¢å¼ºç‰ˆé£ä¹¦å¡ç‰‡"""

    elements = []

    # ========== æ ‡é¢˜åŒº ==========
    now = datetime.now().strftime('%Y-%m-%d %H:%M')

    elements.append({
        "tag": "div",
        "text": {
            "tag": "lark_md",
            "content": f"**ğŸ“Š è®ºæ–‡: {len(papers)} ç¯‡ | åšå®¢: {len(blogs)} ç¯‡**\n**â° {now}**"
        }
    })

    elements.append({"tag": "hr"})

    # ========== è¶‹åŠ¿æ€»ç»“åŒº ==========
    if trend_summary:
        elements.append({
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": f"**ğŸ“ˆ ä»Šæ—¥ç ”ç©¶è¶‹åŠ¿**\n\n{trend_summary}"
            }
        })

        elements.append({"tag": "hr"})

    # ========== ç»å…¸è®ºæ–‡åŒºï¼ˆæ”¾åœ¨å‰é¢ï¼‰ ==========
    if classic_paper:
        elements.append({
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": "**ğŸ“– æ¯æ—¥ç»å…¸è®ºæ–‡æ¨è**"
            }
        })

        # ç»å…¸è®ºæ–‡å†…å®¹ï¼ˆåŒ…å«å…³é”®è¯è§£æï¼‰
        classic_content = f"**{classic_paper['title']}** ({classic_paper.get('year', 'N/A')})\n\n"
        classic_content += f"ğŸ‘¥ **ä½œè€…**: {classic_paper['authors']}\n\n"
        classic_content += f"ğŸ“ **ç®€ä»‹**: {classic_paper['description']}\n\n"

        # æ·»åŠ å…³é”®è¯è§£æ
        keywords = classic_paper.get('keywords', [])
        if keywords:
            classic_content += f"ğŸ”‘ **æ ¸å¿ƒæ¦‚å¿µ**: {', '.join(keywords[:5])}"
            if len(keywords) > 5:
                classic_content += f" ç­‰ {len(keywords)} ä¸ªå…³é”®è¯"
            classic_content += "\n\n"

        elements.append({
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": classic_content
            }
        })

        elements.append({
            "tag": "action",
            "actions": [
                {
                    "tag": "button",
                    "text": {"tag": "plain_text", "content": "æŸ¥çœ‹è®ºæ–‡"},
                    "type": "default",
                    "url": classic_paper['url']
                }
            ]
        })

        elements.append({"tag": "hr"})

    # ========== è®ºæ–‡åŒº ==========
    if papers:
        elements.append({
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": "**ğŸ“š Hugging Face è®ºæ–‡**"
            }
        })

        for i, paper in enumerate(papers, 1):
            # æ ‡é¢˜
            title_text = f"**{i}. {paper['title']}**"
            if paper.get('categories'):
                tags = ' '.join([f"`{cat}`" for cat in paper['categories'][:3]])
                title_text += f"\nğŸ·ï¸  {tags}"

            elements.append({
                "tag": "div",
                "text": {
                    "tag": "lark_md",
                    "content": title_text
                }
            })

            # ä½œè€…å’Œå‘å¸ƒæ—¶é—´
            meta_lines = []
            if paper.get('author_str'):
                meta_lines.append(f"ğŸ‘¥ {paper['author_str']}")
            if paper.get('published'):
                meta_lines.append(f"ğŸ“… {format_datetime(paper['published'])}")

            if meta_lines:
                elements.append({
                    "tag": "div",
                    "text": {
                        "tag": "lark_md",
                        "content": ' | '.join(meta_lines)
                    }
                })

            # æ‘˜è¦ï¼ˆAI è§£è¯»ä¼˜å…ˆï¼‰
            if paper.get('ai_enhanced_summary'):
                summary = paper['ai_enhanced_summary']
                if len(summary) > 600:
                    summary = summary[:600] + '...'
                elements.append({
                    "tag": "div",
                    "text": {
                        "tag": "lark_md",
                        "content": summary
                    }
                })
            elif paper.get('summary'):
                # åŸå§‹æ‘˜è¦
                summary = paper['summary']
                if len(summary) > 300:
                    summary = summary[:300] + '...'
                elements.append({
                    "tag": "div",
                    "text": {
                        "tag": "lark_md",
                        "content": f"ğŸ“ {summary}"
                    }
                })

            # é“¾æ¥æŒ‰é’®
            actions = [
                {
                    "tag": "button",
                    "text": {"tag": "plain_text", "content": "æŸ¥çœ‹è®ºæ–‡"},
                    "type": "default",
                    "url": paper['paper_url']
                }
            ]

            if paper.get('pdf_url'):
                actions.append({
                    "tag": "button",
                    "text": {"tag": "plain_text", "content": "ä¸‹è½½ PDF"},
                    "type": "primary",
                    "url": paper['pdf_url']
                })

            if paper.get('project_page'):
                actions.append({
                    "tag": "button",
                    "text": {"tag": "plain_text", "content": "é¡¹ç›®ä¸»é¡µ"},
                    "type": "default",
                    "url": paper['project_page']
                })

            elements.append({
                "tag": "action",
                "actions": actions
            })

            # åˆ†éš”çº¿
            if i < len(papers):
                elements.append({"tag": "hr"})

    # ========== åšå®¢åŒº ==========
    if blogs:
        elements.append({"tag": "hr"})
        elements.append({
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": "**ğŸ“° å®éªŒå®¤åšå®¢**"
            }
        })

        for blog in blogs[:5]:  # æœ€å¤šæ˜¾ç¤º 5 ç¯‡
            # æ ‡é¢˜å’Œå…ƒä¿¡æ¯
            elements.append({
                "tag": "div",
                "text": {
                    "tag": "lark_md",
                    "content": f"**â€¢ {blog['title']}**\nğŸ¢ {blog['source']} | ğŸ“… {format_datetime(blog['published'])}"
                }
            })

            # é“¾æ¥æŒ‰é’®
            elements.append({
                "tag": "action",
                "actions": [
                    {
                        "tag": "button",
                        "text": {"tag": "plain_text", "content": "é˜…è¯»æ–‡ç« "},
                        "type": "default",
                        "url": blog['link']
                    }
                ]
            })

            # åˆ†éš”çº¿
            if blogs.index(blog) < min(len(blogs), 5) - 1:
                elements.append({"tag": "hr"})

    # ========== æ¨æ–‡åŒº ==========
    if tweets:
        elements.append({"tag": "hr"})
        elements.append({
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": "**ğŸ¦ AI ç ”ç©¶è€…æ¨æ–‡**"
            }
        })

        for tweet in tweets[:5]:
            text = tweet['text']
            if len(text) > 150:
                text = text[:150] + '...'
            tweet_content = f"**@{tweet['username']}**\n{text}\n"
            tweet_content += f"â¤ï¸ {tweet['likes']}  ğŸ”„ {tweet['retweets']}"

            elements.append({
                "tag": "div",
                "text": {
                    "tag": "lark_md",
                    "content": tweet_content
                }
            })

            if tweet.get('link'):
                elements.append({
                    "tag": "action",
                    "actions": [{
                        "tag": "button",
                        "text": {"tag": "plain_text", "content": "æŸ¥çœ‹æ¨æ–‡"},
                        "type": "default",
                        "url": tweet['link']
                    }]
                })

    # ========== æ„å»ºå¡ç‰‡ ==========
    card = {
        "config": {
            "wide_screen_mode": True
        },
        "header": {
            "title": {
                "tag": "plain_text",
                "content": "ğŸ¤– AI Research Daily"
            },
            "template": "blue"
        },
        "elements": elements
    }

    return card


if __name__ == "__main__":
    sys.exit(main())
