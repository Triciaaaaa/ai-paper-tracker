#!/usr/bin/env python3
"""
ğŸ“š Hugging Face Daily Papers æŠ“å–å™¨
è·å– Hugging Face Daily Papers çš„æœ€æ–°è®ºæ–‡
"""

import requests
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import time
import re


class HuggingFacePaperFetcher:
    """Hugging Face Daily Papers æŠ“å–å™¨"""

    BASE_URL = "https://huggingface.co"
    DAILY_PAPERS_API = "/api/daily_papers"

    # é»˜è®¤ç±»åˆ«å…³é”®è¯
    DEFAULT_CATEGORIES = {
        # æ ¸å¿ƒå…³æ³¨é¢†åŸŸ
        'rl_verification': ['reinforcement learning verification', 'verify reinforcement learning', 'formal verification rl', 'safe rl', 'rl safety'],
        'alignment': ['alignment', 'constitutional ai', 'ai safety', 'reward hacking', 'rlhf', 'reward model', 'value learning'],
        'ai4math': ['ai for mathematics', 'mathematical reasoning', 'theorem proving', 'math', 'formal math', 'automated theorem proving'],
        'auto_formalization': ['auto-formalization', 'auto formalization', 'formalization', 'informal to formal', 'proof synthesis', 'formal methods'],
        # ç›¸å…³é¢†åŸŸ
        'reasoning': ['reasoning', 'logic', 'deductive reasoning', 'inductive reasoning', 'chain of thought'],
        'llm': ['large language model', 'llm', 'transformer', 'gpt', 'language model'],
        'reinforcement_learning': ['reinforcement learning', 'rl', 'policy gradient', 'q-learning', 'actor critic'],
        # å…¶ä»–
        'computer_vision': ['vision', 'image', 'video', 'convolutional', 'segmentation', 'detection'],
        'multimodal': ['multimodal', 'vision-language', 'clip', 'visual-language'],
        'generative': ['diffusion', 'gan', 'generation', 'generative'],
        'agents': ['agent', 'autonomous', 'planning', 'decision making']
    }

    def __init__(self, days_back: int = 1, max_papers: int = 50, category_filters: List[str] = None):
        """
        åˆå§‹åŒ–æŠ“å–å™¨

        Args:
            days_back: è·å–æœ€è¿‘å‡ å¤©çš„è®ºæ–‡
            max_papers: æœ€å¤šè·å–å¤šå°‘ç¯‡è®ºæ–‡
            category_filters: ç±»åˆ«è¿‡æ»¤åˆ—è¡¨ï¼Œå¦‚ ['alignment', 'llm']ï¼ŒNone è¡¨ç¤ºä¸è¿‡æ»¤
        """
        self.days_back = days_back
        self.max_papers = max_papers
        self.category_filters = category_filters
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })

    def fetch_papers(self, date: Optional[str] = None) -> List[Dict]:
        """
        è·å–æŒ‡å®šæ—¥æœŸçš„è®ºæ–‡

        Args:
            date: æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD)ï¼Œé»˜è®¤ä¸ºä»Šå¤©

        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        if date is None:
            date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')

        url = f"{self.BASE_URL}{self.DAILY_PAPERS_API}"
        params = {'date': date, 'limit': self.max_papers}

        try:
            print(f"ğŸ“… è·å– {date} çš„ Hugging Face Daily Papers...")
            response = self.session.get(url, params=params, timeout=30)
            response.raise_for_status()

            data = response.json()
            papers = []

            for item in data:
                paper = self._parse_paper(item)
                if paper:
                    papers.append(paper)

            print(f"âœ… è·å–åˆ° {len(papers)} ç¯‡è®ºæ–‡")
            return papers

        except requests.RequestException as e:
            print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
            return []

    def fetch_trending_papers(self) -> List[Dict]:
        """
        è·å–å½“å‰çƒ­é—¨è®ºæ–‡

        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        url = f"{self.BASE_URL}/papers/trending"

        try:
            print("ğŸ”¥ è·å– Hugging Face çƒ­é—¨è®ºæ–‡...")
            # æ³¨æ„ï¼štrending é¡µé¢æ˜¯åŠ¨æ€æ¸²æŸ“çš„ï¼Œè¿™é‡Œä½¿ç”¨ API
            # å®é™…ä¸Š trending æ•°æ®ä¹Ÿåœ¨ daily_papers API ä¸­ï¼Œé€šè¿‡æ’åºè·å–
            response = self.session.get(f"{self.BASE_URL}{self.DAILY_PAPERS_API}",
                                      params={'limit': self.max_papers},
                                      timeout=30)
            response.raise_for_status()

            data = response.json()
            papers = []

            for item in data[:self.max_papers]:
                paper = self._parse_paper(item)
                if paper:
                    papers.append(paper)

            print(f"âœ… è·å–åˆ° {len(papers)} ç¯‡çƒ­é—¨è®ºæ–‡")
            return papers

        except requests.RequestException as e:
            print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
            return []

    def _parse_paper(self, item: Dict) -> Optional[Dict]:
        """
        è§£æè®ºæ–‡æ•°æ®

        Args:
            item: API è¿”å›çš„å•ä¸ªè®ºæ–‡é¡¹

        Returns:
            è§£æåçš„è®ºæ–‡å­—å…¸
        """
        try:
            # API è¿”å›çš„æ•°æ®ç»“æ„ï¼šå¤–å±‚å¯èƒ½åŒ…å« paper å¯¹è±¡ï¼Œä¹Ÿå¯èƒ½ç›´æ¥æ˜¯æ•°æ®
            paper_data = item.get('paper', item)

            # æå–è®ºæ–‡ä¿¡æ¯
            paper_id = paper_data.get('id', '')
            title = paper_data.get('title', '')
            summary = paper_data.get('summary', '')

            # å‘å¸ƒæ—¶é—´ï¼ˆå¯èƒ½æ˜¯ publishedAt æˆ–å…¶ä»–å­—æ®µï¼‰
            published = paper_data.get('publishedAt', paper_data.get('published', paper_data.get('date', '')))

            # æå–ä½œè€…
            authors_list = paper_data.get('authors', [])
            author_names = [a.get('name', '') for a in authors_list if a.get('name')]
            author_list = ', '.join(author_names[:5])  # åªå–å‰5ä¸ªä½œè€…

            # æ„å»ºè®ºæ–‡ URL
            paper_url = f"{self.BASE_URL}/papers/{paper_id}"

            # PDF URLï¼ˆéœ€è¦æ ¹æ® arXiv ID æ„å»ºï¼‰
            # Hugging Face è®ºæ–‡ ID æ ¼å¼é€šå¸¸æ˜¯ "YYMM.NNNNN"ï¼ˆarXiv æ ¼å¼ï¼‰
            pdf_url = f"https://arxiv.org/pdf/{paper_id}.pdf" if paper_id else ''

            # é™„åŠ ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            project_page = paper_data.get('projectPage', '')
            github_repo = paper_data.get('githubRepo', '')
            ai_summary = paper_data.get('ai_summary', '')

            # æ£€æµ‹ç±»åˆ«
            categories = self._detect_categories(title, summary)

            # ç±»åˆ«è¿‡æ»¤
            if self.category_filters and not any(cat in self.category_filters for cat in categories):
                return None  # ä¸åœ¨éœ€è¦çš„ç±»åˆ«ä¸­ï¼Œè·³è¿‡

            return {
                'paper_id': paper_id,
                'title': title,
                'summary': summary,
                'authors': author_names,
                'author_str': author_list,
                'published': published,
                'paper_url': paper_url,
                'pdf_url': pdf_url,
                'project_page': project_page,
                'github_repo': github_repo,
                'ai_summary': ai_summary,
                'categories': categories,
                'source': 'huggingface'
            }

        except Exception as e:
            print(f"âš ï¸  è§£æè®ºæ–‡æ•°æ®å¤±è´¥: {e}")
            return None

    def _detect_categories(self, title: str, summary: str) -> List[str]:
        """
        æ£€æµ‹è®ºæ–‡æ‰€å±ç±»åˆ«

        Args:
            title: è®ºæ–‡æ ‡é¢˜
            summary: è®ºæ–‡æ‘˜è¦

        Returns:
            ç±»åˆ«åˆ—è¡¨
        """
        text = f"{title} {summary}".lower()

        detected = []

        for category, keywords in self.DEFAULT_CATEGORIES.items():
            for keyword in keywords:
                if keyword.lower() in text:
                    detected.append(category)
                    break

        return detected

    def fetch_recent_papers(self) -> List[Dict]:
        """
        è·å–æœ€è¿‘å‡ å¤©çš„è®ºæ–‡

        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        all_papers = []
        seen_ids = set()  # å»é‡

        for i in range(self.days_back):
            date = (datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d')
            papers = self.fetch_papers(date)

            for paper in papers:
                if paper['paper_id'] not in seen_ids:
                    seen_ids.add(paper['paper_id'])
                    all_papers.append(paper)

            if len(all_papers) >= self.max_papers:
                break

            time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«

        return all_papers[:self.max_papers]


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    fetcher = HuggingFacePaperFetcher(days_back=1, max_papers=5)
    papers = fetcher.fetch_recent_papers()

    print("\n" + "=" * 60)
    print("ğŸ“š Hugging Face Daily Papers")
    print("=" * 60)

    for i, paper in enumerate(papers, 1):
        print(f"\n{i}. {paper['title']}")
        print(f"   ä½œè€…: {paper['author_str']}")
        print(f"   å‘å¸ƒ: {paper['published']}")
        print(f"   é“¾æ¥: {paper['paper_url']}")
        print(f"   æ‘˜è¦: {paper['summary'][:100]}...")
